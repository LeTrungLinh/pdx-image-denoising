digraph {
	graph [size="497.09999999999997,497.09999999999997"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140649815371712 [label="
 (1, 3, 64, 64)" fillcolor=darkolivegreen1]
	140649816734448 [label="AddBackward0
------------
alpha: 1"]
	140649816734736 -> 140649816734448
	140649816734736 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649816734592 -> 140649816734736
	140649816734592 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649816734832 -> 140649816734592
	140649816734832 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             32
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649816734976 -> 140649816734832
	140649816734976 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649816735120 -> 140649816734976
	140649816735120 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649816735264 -> 140649816735120
	140649816735264 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             32
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649816735408 -> 140649816735264
	140649816735408 [label="AddBackward0
------------
alpha: 1"]
	140649816735552 -> 140649816735408
	140649816735552 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649816735696 -> 140649816735552
	140649816735696 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815441568 -> 140649816735696
	140649815441568 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815441712 -> 140649815441568
	140649815441712 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815441856 -> 140649815441712
	140649815441856 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815442048 -> 140649815441856
	140649815442048 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             80
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815442192 -> 140649815442048
	140649815442192 [label="CatBackward0
------------
dim: 1"]
	140649815442336 -> 140649815442192
	140649815442336 [label="CatBackward0
------------
dim: 1"]
	140649815442480 -> 140649815442336
	140649815442480 [label="CatBackward0
------------
dim: 1"]
	140649816735504 -> 140649815442480
	140649816735504 [label="AddBackward0
------------
alpha: 1"]
	140649815442720 -> 140649816735504
	140649815442720 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815442864 -> 140649815442720
	140649815442864 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815443008 -> 140649815442864
	140649815443008 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815443152 -> 140649815443008
	140649815443152 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815443296 -> 140649815443152
	140649815443296 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815443488 -> 140649815443296
	140649815443488 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             80
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815443632 -> 140649815443488
	140649815443632 [label="CatBackward0
------------
dim: 1"]
	140649815443776 -> 140649815443632
	140649815443776 [label="CatBackward0
------------
dim: 1"]
	140649815443920 -> 140649815443776
	140649815443920 [label="CatBackward0
------------
dim: 1"]
	140649815442672 -> 140649815443920
	140649815442672 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815444160 -> 140649815442672
	140649815444160 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815444304 -> 140649815444160
	140649815444304 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             96
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815444448 -> 140649815444304
	140649815444448 [label="CatBackward0
------------
dim: 1"]
	140649815444592 -> 140649815444448
	140649815444592 [label="AddBackward0
------------
alpha: 1"]
	140649815444736 -> 140649815444592
	140649815444736 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815444880 -> 140649815444736
	140649815444880 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815445024 -> 140649815444880
	140649815445024 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815445168 -> 140649815445024
	140649815445168 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815445312 -> 140649815445168
	140649815445312 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815445504 -> 140649815445312
	140649815445504 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             80
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815445648 -> 140649815445504
	140649815445648 [label="CatBackward0
------------
dim: 1"]
	140649815445792 -> 140649815445648
	140649815445792 [label="CatBackward0
------------
dim: 1"]
	140649815445936 -> 140649815445792
	140649815445936 [label="CatBackward0
------------
dim: 1"]
	140649815444688 -> 140649815445936
	140649815444688 [label="AddBackward0
------------
alpha: 1"]
	140649815446176 -> 140649815444688
	140649815446176 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815446320 -> 140649815446176
	140649815446320 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815446464 -> 140649815446320
	140649815446464 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815446608 -> 140649815446464
	140649815446608 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815446752 -> 140649815446608
	140649815446752 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815446944 -> 140649815446752
	140649815446944 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             80
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815447088 -> 140649815446944
	140649815447088 [label="CatBackward0
------------
dim: 1"]
	140649815447232 -> 140649815447088
	140649815447232 [label="CatBackward0
------------
dim: 1"]
	140649815447376 -> 140649815447232
	140649815447376 [label="CatBackward0
------------
dim: 1"]
	140649815446128 -> 140649815447376
	140649815446128 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815447616 -> 140649815446128
	140649815447616 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815447760 -> 140649815447616
	140649815447760 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             32
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815447904 -> 140649815447760
	140649815447904 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815448048 -> 140649815447904
	140649815448048 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815448192 -> 140649815448048
	140649815448192 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              3
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815448336 -> 140649815448192
	140649816186352 [label="input_block.conv_1.depthwise.weight
 (3, 1, 3, 3)" fillcolor=lightblue]
	140649816186352 -> 140649815448336
	140649815448336 [label=AccumulateGrad]
	140649815448144 -> 140649815448048
	140649816186512 [label="input_block.conv_1.pointwise.weight
 (32, 3, 1, 1)" fillcolor=lightblue]
	140649816186512 -> 140649815448144
	140649815448144 [label=AccumulateGrad]
	140649815448000 -> 140649815447904
	140649816186752 [label="input_block.actv_1.weight
 (32)" fillcolor=lightblue]
	140649816186752 -> 140649815448000
	140649815448000 [label=AccumulateGrad]
	140649815447856 -> 140649815447760
	140649816186672 [label="input_block.conv_2.depthwise.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	140649816186672 -> 140649815447856
	140649815447856 [label=AccumulateGrad]
	140649815447712 -> 140649815447616
	140649816186832 [label="input_block.conv_2.pointwise.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	140649816186832 -> 140649815447712
	140649815447712 [label=AccumulateGrad]
	140649815447568 -> 140649815446128
	140649816186912 [label="input_block.actv_2.weight
 (32)" fillcolor=lightblue]
	140649816186912 -> 140649815447568
	140649815447568 [label=AccumulateGrad]
	140649815447520 -> 140649815447376
	140649815447520 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815448432 -> 140649815447520
	140649815448432 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815447952 -> 140649815448432
	140649815447952 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815448480 -> 140649815447952
	140649815448480 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             32
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815446128 -> 140649815448480
	140649815448624 -> 140649815448480
	140649816187072 [label="block_0_0.conv_0.depthwise.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	140649816187072 -> 140649815448624
	140649815448624 [label=AccumulateGrad]
	140649815448240 -> 140649815447952
	140649816187232 [label="block_0_0.conv_0.pointwise.weight
 (16, 32, 1, 1)" fillcolor=lightblue]
	140649816187232 -> 140649815448240
	140649815448240 [label=AccumulateGrad]
	140649815447808 -> 140649815448432
	140649816188272 [label="block_0_0.bn_0.weight
 (16)" fillcolor=lightblue]
	140649816188272 -> 140649815447808
	140649815447808 [label=AccumulateGrad]
	140649815447664 -> 140649815448432
	140649816188352 [label="block_0_0.bn_0.bias
 (16)" fillcolor=lightblue]
	140649816188352 -> 140649815447664
	140649815447664 [label=AccumulateGrad]
	140649815448384 -> 140649815447520
	140649816190272 [label="block_0_0.actv_0.weight
 (16)" fillcolor=lightblue]
	140649816190272 -> 140649815448384
	140649815448384 [label=AccumulateGrad]
	140649815447328 -> 140649815447232
	140649815447328 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815448672 -> 140649815447328
	140649815448672 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815448720 -> 140649815448672
	140649815448720 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815448816 -> 140649815448720
	140649815448816 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             48
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815447376 -> 140649815448816
	140649815448960 -> 140649815448816
	140649816187392 [label="block_0_0.conv_1.depthwise.weight
 (48, 1, 3, 3)" fillcolor=lightblue]
	140649816187392 -> 140649815448960
	140649815448960 [label=AccumulateGrad]
	140649815448768 -> 140649815448720
	140649816187552 [label="block_0_0.conv_1.pointwise.weight
 (16, 48, 1, 1)" fillcolor=lightblue]
	140649816187552 -> 140649815448768
	140649815448768 [label=AccumulateGrad]
	140649815448096 -> 140649815448672
	140649816188832 [label="block_0_0.bn_1.weight
 (16)" fillcolor=lightblue]
	140649816188832 -> 140649815448096
	140649815448096 [label=AccumulateGrad]
	140649815448288 -> 140649815448672
	140649816188912 [label="block_0_0.bn_1.bias
 (16)" fillcolor=lightblue]
	140649816188912 -> 140649815448288
	140649815448288 [label=AccumulateGrad]
	140649815447424 -> 140649815447328
	140649816190352 [label="block_0_0.actv_1.weight
 (16)" fillcolor=lightblue]
	140649816190352 -> 140649815447424
	140649815447424 [label=AccumulateGrad]
	140649815447184 -> 140649815447088
	140649815447184 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815449008 -> 140649815447184
	140649815449008 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815449056 -> 140649815449008
	140649815449056 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815449152 -> 140649815449056
	140649815449152 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             64
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815447232 -> 140649815449152
	140649815449296 -> 140649815449152
	140649816187712 [label="block_0_0.conv_2.depthwise.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	140649816187712 -> 140649815449296
	140649815449296 [label=AccumulateGrad]
	140649815449104 -> 140649815449056
	140649816187872 [label="block_0_0.conv_2.pointwise.weight
 (16, 64, 1, 1)" fillcolor=lightblue]
	140649816187872 -> 140649815449104
	140649815449104 [label=AccumulateGrad]
	140649815448576 -> 140649815449008
	140649816189312 [label="block_0_0.bn_2.weight
 (16)" fillcolor=lightblue]
	140649816189312 -> 140649815448576
	140649815448576 [label=AccumulateGrad]
	140649815448528 -> 140649815449008
	140649816189392 [label="block_0_0.bn_2.bias
 (16)" fillcolor=lightblue]
	140649816189392 -> 140649815448528
	140649815448528 [label=AccumulateGrad]
	140649815447472 -> 140649815447184
	140649816190432 [label="block_0_0.actv_2.weight
 (16)" fillcolor=lightblue]
	140649816190432 -> 140649815447472
	140649815447472 [label=AccumulateGrad]
	140649815447040 -> 140649815446944
	140649816188032 [label="block_0_0.conv_3.depthwise.weight
 (80, 1, 3, 3)" fillcolor=lightblue]
	140649816188032 -> 140649815447040
	140649815447040 [label=AccumulateGrad]
	140649815446896 -> 140649815446752
	140649816188192 [label="block_0_0.conv_3.pointwise.weight
 (32, 80, 1, 1)" fillcolor=lightblue]
	140649816188192 -> 140649815446896
	140649815446896 [label=AccumulateGrad]
	140649815446704 -> 140649815446608
	140649816189792 [label="block_0_0.bn_3.weight
 (32)" fillcolor=lightblue]
	140649816189792 -> 140649815446704
	140649815446704 [label=AccumulateGrad]
	140649815446656 -> 140649815446608
	140649816189872 [label="block_0_0.bn_3.bias
 (32)" fillcolor=lightblue]
	140649816189872 -> 140649815446656
	140649815446656 [label=AccumulateGrad]
	140649815446560 -> 140649815446464
	140649816190512 [label="block_0_0.actv_3.weight
 (32)" fillcolor=lightblue]
	140649816190512 -> 140649815446560
	140649815446560 [label=AccumulateGrad]
	140649815446416 -> 140649815446320
	140649815446416 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (1, 32, 1, 1)"]
	140649815446848 -> 140649815446416
	140649815446848 [label="UnsqueezeBackward0
------------------
dim: 3"]
	140649815449344 -> 140649815446848
	140649815449344 [label="UnsqueezeBackward0
------------------
dim: 2"]
	140649815447136 -> 140649815449344
	140649815447136 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649815448912 -> 140649815447136
	140649815448912 [label="SumBackward1
--------------------------
dim           :       (0,)
keepdim       :      False
self_sym_sizes: (2, 1, 32)"]
	140649815449248 -> 140649815448912
	140649815449248 [label="StackBackward0
--------------
dim: 0"]
	140649815449440 -> 140649815449248
	140649815449440 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :         (1, 2)
mat1_sym_strides:         (2, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (2, 32)
mat2_sym_strides:         (1, 2)"]
	140649815449584 -> 140649815449440
	140649816190832 [label="block_0_0.cbam.channel_attention.shared_mlp.3.bias
 (32)" fillcolor=lightblue]
	140649816190832 -> 140649815449584
	140649815449584 [label=AccumulateGrad]
	140649815449536 -> 140649815449440
	140649815449536 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649815449680 -> 140649815449536
	140649815449680 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :        (1, 32)
mat1_sym_strides:        (32, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (32, 2)
mat2_sym_strides:        (1, 32)"]
	140649815449872 -> 140649815449680
	140649816190672 [label="block_0_0.cbam.channel_attention.shared_mlp.1.bias
 (2)" fillcolor=lightblue]
	140649816190672 -> 140649815449872
	140649815449872 [label=AccumulateGrad]
	140649815449824 -> 140649815449680
	140649815449824 [label="ReshapeAliasBackward0
-----------------------------
self_sym_sizes: (1, 32, 1, 1)"]
	140649815449968 -> 140649815449824
	140649815449968 [label="AvgPool2DBackward0
---------------------------------
ceil_mode        :          False
count_include_pad:           True
divisor_override :           None
kernel_size      :       (64, 64)
padding          :         (0, 0)
self             : [saved tensor]
stride           :       (64, 64)"]
	140649815446464 -> 140649815449968
	140649815449776 -> 140649815449680
	140649815449776 [label=TBackward0]
	140649815450112 -> 140649815449776
	140649816190592 [label="block_0_0.cbam.channel_attention.shared_mlp.1.weight
 (2, 32)" fillcolor=lightblue]
	140649816190592 -> 140649815450112
	140649815450112 [label=AccumulateGrad]
	140649815449488 -> 140649815449440
	140649815449488 [label=TBackward0]
	140649815450160 -> 140649815449488
	140649816190752 [label="block_0_0.cbam.channel_attention.shared_mlp.3.weight
 (32, 2)" fillcolor=lightblue]
	140649816190752 -> 140649815450160
	140649815450160 [label=AccumulateGrad]
	140649815449200 -> 140649815449248
	140649815449200 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :         (1, 2)
mat1_sym_strides:         (2, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (2, 32)
mat2_sym_strides:         (1, 2)"]
	140649815449584 -> 140649815449200
	140649815450064 -> 140649815449200
	140649815450064 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649815450016 -> 140649815450064
	140649815450016 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :        (1, 32)
mat1_sym_strides:        (32, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (32, 2)
mat2_sym_strides:        (1, 32)"]
	140649815449872 -> 140649815450016
	140649815450304 -> 140649815450016
	140649815450304 [label="ReshapeAliasBackward0
-----------------------------
self_sym_sizes: (1, 32, 1, 1)"]
	140649815450448 -> 140649815450304
	140649815450448 [label="MaxPool2DWithIndicesBackward0
-----------------------------
ceil_mode  :          False
dilation   :         (1, 1)
kernel_size:       (64, 64)
padding    :         (0, 0)
result1    : [saved tensor]
self       : [saved tensor]
stride     :       (64, 64)"]
	140649815446464 -> 140649815450448
	140649815450256 -> 140649815450016
	140649815450256 [label=TBackward0]
	140649815450112 -> 140649815450256
	140649815449920 -> 140649815449200
	140649815449920 [label=TBackward0]
	140649815450160 -> 140649815449920
	140649815446272 -> 140649815446176
	140649815446272 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649815446800 -> 140649815446272
	140649815446800 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815448864 -> 140649815446800
	140649815448864 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (3, 3)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815450496 -> 140649815448864
	140649815450496 [label="CatBackward0
------------
dim: 1"]
	140649815450544 -> 140649815450496
	140649815450544 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649815450592 -> 140649815450544
	140649815450592 [label="MaxBackward0
---------------------------
dim       :               1
indices   :  [saved tensor]
keepdim   :           False
self_sizes: (1, 32, 64, 64)"]
	140649815446320 -> 140649815450592
	140649815449728 -> 140649815450496
	140649815449728 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649815450640 -> 140649815449728
	140649815450640 [label="MeanBackward1
-------------------------------
dim           :            (1,)
keepdim       :           False
self          :  [saved tensor]
self_sym_sizes: (1, 32, 64, 64)"]
	140649815446320 -> 140649815450640
	140649815449632 -> 140649815448864
	140649816190992 [label="block_0_0.cbam.spatial_attention.spatial_attention.0.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	140649816190992 -> 140649815449632
	140649815449632 [label=AccumulateGrad]
	140649815446992 -> 140649815446800
	140649816190912 [label="block_0_0.cbam.spatial_attention.spatial_attention.1.weight
 (1)" fillcolor=lightblue]
	140649816190912 -> 140649815446992
	140649815446992 [label=AccumulateGrad]
	140649815446368 -> 140649815446800
	140649816191072 [label="block_0_0.cbam.spatial_attention.spatial_attention.1.bias
 (1)" fillcolor=lightblue]
	140649816191072 -> 140649815446368
	140649815446368 [label=AccumulateGrad]
	140649815446128 -> 140649815444688
	140649815446080 -> 140649815445936
	140649815446080 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815447280 -> 140649815446080
	140649815447280 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815450736 -> 140649815447280
	140649815450736 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815450352 -> 140649815450736
	140649815450352 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             32
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815444688 -> 140649815450352
	140649815450880 -> 140649815450352
	140649816191552 [label="block_0_1.conv_0.depthwise.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	140649816191552 -> 140649815450880
	140649815450880 [label=AccumulateGrad]
	140649815450400 -> 140649815450736
	140649816191712 [label="block_0_1.conv_0.pointwise.weight
 (16, 32, 1, 1)" fillcolor=lightblue]
	140649816191712 -> 140649815450400
	140649815450400 [label=AccumulateGrad]
	140649815446512 -> 140649815447280
	140649816192752 [label="block_0_1.bn_0.weight
 (16)" fillcolor=lightblue]
	140649816192752 -> 140649815446512
	140649815446512 [label=AccumulateGrad]
	140649815449392 -> 140649815447280
	140649816192832 [label="block_0_1.bn_0.bias
 (16)" fillcolor=lightblue]
	140649816192832 -> 140649815449392
	140649815449392 [label=AccumulateGrad]
	140649815446224 -> 140649815446080
	140649816194672 [label="block_0_1.actv_0.weight
 (16)" fillcolor=lightblue]
	140649816194672 -> 140649815446224
	140649815446224 [label=AccumulateGrad]
	140649815445888 -> 140649815445792
	140649815445888 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815450928 -> 140649815445888
	140649815450928 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815450976 -> 140649815450928
	140649815450976 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815451072 -> 140649815450976
	140649815451072 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             48
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815445936 -> 140649815451072
	140649815451216 -> 140649815451072
	140649816191872 [label="block_0_1.conv_1.depthwise.weight
 (48, 1, 3, 3)" fillcolor=lightblue]
	140649816191872 -> 140649815451216
	140649815451216 [label=AccumulateGrad]
	140649815451024 -> 140649815450976
	140649816192032 [label="block_0_1.conv_1.pointwise.weight
 (16, 48, 1, 1)" fillcolor=lightblue]
	140649816192032 -> 140649815451024
	140649815451024 [label=AccumulateGrad]
	140649815450688 -> 140649815450928
	140649816193232 [label="block_0_1.bn_1.weight
 (16)" fillcolor=lightblue]
	140649816193232 -> 140649815450688
	140649815450688 [label=AccumulateGrad]
	140649815450208 -> 140649815450928
	140649816193312 [label="block_0_1.bn_1.bias
 (16)" fillcolor=lightblue]
	140649816193312 -> 140649815450208
	140649815450208 [label=AccumulateGrad]
	140649815445984 -> 140649815445888
	140649816194752 [label="block_0_1.actv_1.weight
 (16)" fillcolor=lightblue]
	140649816194752 -> 140649815445984
	140649815445984 [label=AccumulateGrad]
	140649815445744 -> 140649815445648
	140649815445744 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815451264 -> 140649815445744
	140649815451264 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815451312 -> 140649815451264
	140649815451312 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815451408 -> 140649815451312
	140649815451408 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             64
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815445792 -> 140649815451408
	140649815451552 -> 140649815451408
	140649816192192 [label="block_0_1.conv_2.depthwise.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	140649816192192 -> 140649815451552
	140649815451552 [label=AccumulateGrad]
	140649815451360 -> 140649815451312
	140649816192352 [label="block_0_1.conv_2.pointwise.weight
 (16, 64, 1, 1)" fillcolor=lightblue]
	140649816192352 -> 140649815451360
	140649815451360 [label=AccumulateGrad]
	140649815450832 -> 140649815451264
	140649816193712 [label="block_0_1.bn_2.weight
 (16)" fillcolor=lightblue]
	140649816193712 -> 140649815450832
	140649815450832 [label=AccumulateGrad]
	140649815450784 -> 140649815451264
	140649816193792 [label="block_0_1.bn_2.bias
 (16)" fillcolor=lightblue]
	140649816193792 -> 140649815450784
	140649815450784 [label=AccumulateGrad]
	140649815446032 -> 140649815445744
	140649816194832 [label="block_0_1.actv_2.weight
 (16)" fillcolor=lightblue]
	140649816194832 -> 140649815446032
	140649815446032 [label=AccumulateGrad]
	140649815445600 -> 140649815445504
	140649816192512 [label="block_0_1.conv_3.depthwise.weight
 (80, 1, 3, 3)" fillcolor=lightblue]
	140649816192512 -> 140649815445600
	140649815445600 [label=AccumulateGrad]
	140649815445456 -> 140649815445312
	140649816192672 [label="block_0_1.conv_3.pointwise.weight
 (32, 80, 1, 1)" fillcolor=lightblue]
	140649816192672 -> 140649815445456
	140649815445456 [label=AccumulateGrad]
	140649815445264 -> 140649815445168
	140649816194192 [label="block_0_1.bn_3.weight
 (32)" fillcolor=lightblue]
	140649816194192 -> 140649815445264
	140649815445264 [label=AccumulateGrad]
	140649815445216 -> 140649815445168
	140649816194272 [label="block_0_1.bn_3.bias
 (32)" fillcolor=lightblue]
	140649816194272 -> 140649815445216
	140649815445216 [label=AccumulateGrad]
	140649815445120 -> 140649815445024
	140649816194912 [label="block_0_1.actv_3.weight
 (32)" fillcolor=lightblue]
	140649816194912 -> 140649815445120
	140649815445120 [label=AccumulateGrad]
	140649815444976 -> 140649815444880
	140649815444976 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (1, 32, 1, 1)"]
	140649815445408 -> 140649815444976
	140649815445408 [label="UnsqueezeBackward0
------------------
dim: 3"]
	140649815451600 -> 140649815445408
	140649815451600 [label="UnsqueezeBackward0
------------------
dim: 2"]
	140649815445696 -> 140649815451600
	140649815445696 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649815451168 -> 140649815445696
	140649815451168 [label="SumBackward1
--------------------------
dim           :       (0,)
keepdim       :      False
self_sym_sizes: (2, 1, 32)"]
	140649815451504 -> 140649815451168
	140649815451504 [label="StackBackward0
--------------
dim: 0"]
	140649815451696 -> 140649815451504
	140649815451696 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :         (1, 2)
mat1_sym_strides:         (2, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (2, 32)
mat2_sym_strides:         (1, 2)"]
	140649815451888 -> 140649815451696
	140649816391904 [label="block_0_1.cbam.channel_attention.shared_mlp.3.bias
 (32)" fillcolor=lightblue]
	140649816391904 -> 140649815451888
	140649815451888 [label=AccumulateGrad]
	140649815451840 -> 140649815451696
	140649815451840 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649815451984 -> 140649815451840
	140649815451984 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :        (1, 32)
mat1_sym_strides:        (32, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (32, 2)
mat2_sym_strides:        (1, 32)"]
	140649815452176 -> 140649815451984
	140649816391744 [label="block_0_1.cbam.channel_attention.shared_mlp.1.bias
 (2)" fillcolor=lightblue]
	140649816391744 -> 140649815452176
	140649815452176 [label=AccumulateGrad]
	140649815452128 -> 140649815451984
	140649815452128 [label="ReshapeAliasBackward0
-----------------------------
self_sym_sizes: (1, 32, 1, 1)"]
	140649815452272 -> 140649815452128
	140649815452272 [label="AvgPool2DBackward0
---------------------------------
ceil_mode        :          False
count_include_pad:           True
divisor_override :           None
kernel_size      :       (64, 64)
padding          :         (0, 0)
self             : [saved tensor]
stride           :       (64, 64)"]
	140649815445024 -> 140649815452272
	140649815452080 -> 140649815451984
	140649815452080 [label=TBackward0]
	140649815452416 -> 140649815452080
	140649816194992 [label="block_0_1.cbam.channel_attention.shared_mlp.1.weight
 (2, 32)" fillcolor=lightblue]
	140649816194992 -> 140649815452416
	140649815452416 [label=AccumulateGrad]
	140649815451792 -> 140649815451696
	140649815451792 [label=TBackward0]
	140649815452464 -> 140649815451792
	140649816391824 [label="block_0_1.cbam.channel_attention.shared_mlp.3.weight
 (32, 2)" fillcolor=lightblue]
	140649816391824 -> 140649815452464
	140649815452464 [label=AccumulateGrad]
	140649815451456 -> 140649815451504
	140649815451456 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :         (1, 2)
mat1_sym_strides:         (2, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (2, 32)
mat2_sym_strides:         (1, 2)"]
	140649815451888 -> 140649815451456
	140649815452368 -> 140649815451456
	140649815452368 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649815452320 -> 140649815452368
	140649815452320 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :        (1, 32)
mat1_sym_strides:        (32, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (32, 2)
mat2_sym_strides:        (1, 32)"]
	140649815452176 -> 140649815452320
	140649815452608 -> 140649815452320
	140649815452608 [label="ReshapeAliasBackward0
-----------------------------
self_sym_sizes: (1, 32, 1, 1)"]
	140649815452752 -> 140649815452608
	140649815452752 [label="MaxPool2DWithIndicesBackward0
-----------------------------
ceil_mode  :          False
dilation   :         (1, 1)
kernel_size:       (64, 64)
padding    :         (0, 0)
result1    : [saved tensor]
self       : [saved tensor]
stride     :       (64, 64)"]
	140649815445024 -> 140649815452752
	140649815452560 -> 140649815452320
	140649815452560 [label=TBackward0]
	140649815452416 -> 140649815452560
	140649815452224 -> 140649815451456
	140649815452224 [label=TBackward0]
	140649815452464 -> 140649815452224
	140649815444832 -> 140649815444736
	140649815444832 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649815445360 -> 140649815444832
	140649815445360 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815451120 -> 140649815445360
	140649815451120 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (3, 3)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815452800 -> 140649815451120
	140649815452800 [label="CatBackward0
------------
dim: 1"]
	140649815452848 -> 140649815452800
	140649815452848 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649815452896 -> 140649815452848
	140649815452896 [label="MaxBackward0
---------------------------
dim       :               1
indices   :  [saved tensor]
keepdim   :           False
self_sizes: (1, 32, 64, 64)"]
	140649815444880 -> 140649815452896
	140649815452032 -> 140649815452800
	140649815452032 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649815452944 -> 140649815452032
	140649815452944 [label="MeanBackward1
-------------------------------
dim           :            (1,)
keepdim       :           False
self          :  [saved tensor]
self_sym_sizes: (1, 32, 64, 64)"]
	140649815444880 -> 140649815452944
	140649815451936 -> 140649815451120
	140649816392144 [label="block_0_1.cbam.spatial_attention.spatial_attention.0.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	140649816392144 -> 140649815451936
	140649815451936 [label=AccumulateGrad]
	140649815445552 -> 140649815445360
	140649816392064 [label="block_0_1.cbam.spatial_attention.spatial_attention.1.weight
 (1)" fillcolor=lightblue]
	140649816392064 -> 140649815445552
	140649815445552 [label=AccumulateGrad]
	140649815444928 -> 140649815445360
	140649816392224 [label="block_0_1.cbam.spatial_attention.spatial_attention.1.bias
 (1)" fillcolor=lightblue]
	140649816392224 -> 140649815444928
	140649815444928 [label=AccumulateGrad]
	140649815444688 -> 140649815444592
	140649815444544 -> 140649815444448
	140649815444544 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815445840 -> 140649815444544
	140649815445840 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (2, 2)
transposed        :           True
weight            : [saved tensor]"]
	140649815453040 -> 140649815445840
	140649815453040 [label="AddBackward0
------------
alpha: 1"]
	140649815452656 -> 140649815453040
	140649815452656 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815453184 -> 140649815452656
	140649815453184 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815453328 -> 140649815453184
	140649815453328 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815453472 -> 140649815453328
	140649815453472 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815453616 -> 140649815453472
	140649815453616 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815453808 -> 140649815453616
	140649815453808 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            160
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815453952 -> 140649815453808
	140649815453952 [label="CatBackward0
------------
dim: 1"]
	140649815454096 -> 140649815453952
	140649815454096 [label="CatBackward0
------------
dim: 1"]
	140649815454240 -> 140649815454096
	140649815454240 [label="CatBackward0
------------
dim: 1"]
	140649815452704 -> 140649815454240
	140649815452704 [label="AddBackward0
------------
alpha: 1"]
	140649815454480 -> 140649815452704
	140649815454480 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815454624 -> 140649815454480
	140649815454624 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815454768 -> 140649815454624
	140649815454768 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815454912 -> 140649815454768
	140649815454912 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815455056 -> 140649815454912
	140649815455056 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815455248 -> 140649815455056
	140649815455248 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            160
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815455392 -> 140649815455248
	140649815455392 [label="CatBackward0
------------
dim: 1"]
	140649815455536 -> 140649815455392
	140649815455536 [label="CatBackward0
------------
dim: 1"]
	140649815455680 -> 140649815455536
	140649815455680 [label="CatBackward0
------------
dim: 1"]
	140649815454432 -> 140649815455680
	140649815454432 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815455920 -> 140649815454432
	140649815455920 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815456064 -> 140649815455920
	140649815456064 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            192
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815456208 -> 140649815456064
	140649815456208 [label="CatBackward0
------------
dim: 1"]
	140649815456352 -> 140649815456208
	140649815456352 [label="AddBackward0
------------
alpha: 1"]
	140649815456496 -> 140649815456352
	140649815456496 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815456640 -> 140649815456496
	140649815456640 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815456784 -> 140649815456640
	140649815456784 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815456928 -> 140649815456784
	140649815456928 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815457072 -> 140649815456928
	140649815457072 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815457264 -> 140649815457072
	140649815457264 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            160
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815457408 -> 140649815457264
	140649815457408 [label="CatBackward0
------------
dim: 1"]
	140649815457552 -> 140649815457408
	140649815457552 [label="CatBackward0
------------
dim: 1"]
	140649815457696 -> 140649815457552
	140649815457696 [label="CatBackward0
------------
dim: 1"]
	140649815456448 -> 140649815457696
	140649815456448 [label="AddBackward0
------------
alpha: 1"]
	140649815539920 -> 140649815456448
	140649815539920 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815540064 -> 140649815539920
	140649815540064 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815540208 -> 140649815540064
	140649815540208 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815540352 -> 140649815540208
	140649815540352 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815540496 -> 140649815540352
	140649815540496 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815540688 -> 140649815540496
	140649815540688 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            160
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815540832 -> 140649815540688
	140649815540832 [label="CatBackward0
------------
dim: 1"]
	140649815540976 -> 140649815540832
	140649815540976 [label="CatBackward0
------------
dim: 1"]
	140649815541120 -> 140649815540976
	140649815541120 [label="CatBackward0
------------
dim: 1"]
	140649815539872 -> 140649815541120
	140649815539872 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815541360 -> 140649815539872
	140649815541360 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	140649815444592 -> 140649815541360
	140649815541504 -> 140649815541360
	140649816392624 [label="down_0.conv.weight
 (64, 32, 2, 2)" fillcolor=lightblue]
	140649816392624 -> 140649815541504
	140649815541504 [label=AccumulateGrad]
	140649815541456 -> 140649815541360
	140649816392704 [label="down_0.conv.bias
 (64)" fillcolor=lightblue]
	140649816392704 -> 140649815541456
	140649815541456 [label=AccumulateGrad]
	140649815541312 -> 140649815539872
	140649816392784 [label="down_0.actv.weight
 (64)" fillcolor=lightblue]
	140649816392784 -> 140649815541312
	140649815541312 [label=AccumulateGrad]
	140649815541264 -> 140649815541120
	140649815541264 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815541552 -> 140649815541264
	140649815541552 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815541696 -> 140649815541552
	140649815541696 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815541888 -> 140649815541696
	140649815541888 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             64
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815539872 -> 140649815541888
	140649815542032 -> 140649815541888
	140649816392944 [label="block_1_0.conv_0.depthwise.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	140649816392944 -> 140649815542032
	140649815542032 [label=AccumulateGrad]
	140649815541840 -> 140649815541696
	140649816393104 [label="block_1_0.conv_0.pointwise.weight
 (32, 64, 1, 1)" fillcolor=lightblue]
	140649816393104 -> 140649815541840
	140649815541840 [label=AccumulateGrad]
	140649815541648 -> 140649815541552
	140649816394144 [label="block_1_0.bn_0.weight
 (32)" fillcolor=lightblue]
	140649816394144 -> 140649815541648
	140649815541648 [label=AccumulateGrad]
	140649815541408 -> 140649815541552
	140649816394224 [label="block_1_0.bn_0.bias
 (32)" fillcolor=lightblue]
	140649816394224 -> 140649815541408
	140649815541408 [label=AccumulateGrad]
	140649815541600 -> 140649815541264
	140649816395904 [label="block_1_0.actv_0.weight
 (32)" fillcolor=lightblue]
	140649816395904 -> 140649815541600
	140649815541600 [label=AccumulateGrad]
	140649815541072 -> 140649815540976
	140649815541072 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815542080 -> 140649815541072
	140649815542080 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815542128 -> 140649815542080
	140649815542128 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815542224 -> 140649815542128
	140649815542224 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             96
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815541120 -> 140649815542224
	140649815542368 -> 140649815542224
	140649816393264 [label="block_1_0.conv_1.depthwise.weight
 (96, 1, 3, 3)" fillcolor=lightblue]
	140649816393264 -> 140649815542368
	140649815542368 [label=AccumulateGrad]
	140649815542176 -> 140649815542128
	140649816393424 [label="block_1_0.conv_1.pointwise.weight
 (32, 96, 1, 1)" fillcolor=lightblue]
	140649816393424 -> 140649815542176
	140649815542176 [label=AccumulateGrad]
	140649815541744 -> 140649815542080
	140649816394624 [label="block_1_0.bn_1.weight
 (32)" fillcolor=lightblue]
	140649816394624 -> 140649815541744
	140649815541744 [label=AccumulateGrad]
	140649815541792 -> 140649815542080
	140649816394704 [label="block_1_0.bn_1.bias
 (32)" fillcolor=lightblue]
	140649816394704 -> 140649815541792
	140649815541792 [label=AccumulateGrad]
	140649815541168 -> 140649815541072
	140649816395984 [label="block_1_0.actv_1.weight
 (32)" fillcolor=lightblue]
	140649816395984 -> 140649815541168
	140649815541168 [label=AccumulateGrad]
	140649815540928 -> 140649815540832
	140649815540928 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815542416 -> 140649815540928
	140649815542416 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815542464 -> 140649815542416
	140649815542464 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815542560 -> 140649815542464
	140649815542560 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            128
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815540976 -> 140649815542560
	140649815542704 -> 140649815542560
	140649816393584 [label="block_1_0.conv_2.depthwise.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	140649816393584 -> 140649815542704
	140649815542704 [label=AccumulateGrad]
	140649815542512 -> 140649815542464
	140649816393744 [label="block_1_0.conv_2.pointwise.weight
 (32, 128, 1, 1)" fillcolor=lightblue]
	140649816393744 -> 140649815542512
	140649815542512 [label=AccumulateGrad]
	140649815541984 -> 140649815542416
	140649816395024 [label="block_1_0.bn_2.weight
 (32)" fillcolor=lightblue]
	140649816395024 -> 140649815541984
	140649815541984 [label=AccumulateGrad]
	140649815541936 -> 140649815542416
	140649816395104 [label="block_1_0.bn_2.bias
 (32)" fillcolor=lightblue]
	140649816395104 -> 140649815541936
	140649815541936 [label=AccumulateGrad]
	140649815541216 -> 140649815540928
	140649816396064 [label="block_1_0.actv_2.weight
 (32)" fillcolor=lightblue]
	140649816396064 -> 140649815541216
	140649815541216 [label=AccumulateGrad]
	140649815540784 -> 140649815540688
	140649816393904 [label="block_1_0.conv_3.depthwise.weight
 (160, 1, 3, 3)" fillcolor=lightblue]
	140649816393904 -> 140649815540784
	140649815540784 [label=AccumulateGrad]
	140649815540640 -> 140649815540496
	140649816394064 [label="block_1_0.conv_3.pointwise.weight
 (64, 160, 1, 1)" fillcolor=lightblue]
	140649816394064 -> 140649815540640
	140649815540640 [label=AccumulateGrad]
	140649815540448 -> 140649815540352
	140649816395264 [label="block_1_0.bn_3.weight
 (64)" fillcolor=lightblue]
	140649816395264 -> 140649815540448
	140649815540448 [label=AccumulateGrad]
	140649815540400 -> 140649815540352
	140649816395504 [label="block_1_0.bn_3.bias
 (64)" fillcolor=lightblue]
	140649816395504 -> 140649815540400
	140649815540400 [label=AccumulateGrad]
	140649815540304 -> 140649815540208
	140649816396144 [label="block_1_0.actv_3.weight
 (64)" fillcolor=lightblue]
	140649816396144 -> 140649815540304
	140649815540304 [label=AccumulateGrad]
	140649815540160 -> 140649815540064
	140649815540160 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (1, 64, 1, 1)"]
	140649815540592 -> 140649815540160
	140649815540592 [label="UnsqueezeBackward0
------------------
dim: 3"]
	140649815542752 -> 140649815540592
	140649815542752 [label="UnsqueezeBackward0
------------------
dim: 2"]
	140649815540880 -> 140649815542752
	140649815540880 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649815542320 -> 140649815540880
	140649815542320 [label="SumBackward1
--------------------------
dim           :       (0,)
keepdim       :      False
self_sym_sizes: (2, 1, 64)"]
	140649815542656 -> 140649815542320
	140649815542656 [label="StackBackward0
--------------
dim: 0"]
	140649815542848 -> 140649815542656
	140649815542848 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :         (1, 4)
mat1_sym_strides:         (4, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (4, 64)
mat2_sym_strides:         (1, 4)"]
	140649815542992 -> 140649815542848
	140649816396464 [label="block_1_0.cbam.channel_attention.shared_mlp.3.bias
 (64)" fillcolor=lightblue]
	140649816396464 -> 140649815542992
	140649815542992 [label=AccumulateGrad]
	140649815542944 -> 140649815542848
	140649815542944 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649815543088 -> 140649815542944
	140649815543088 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :        (1, 64)
mat1_sym_strides:        (64, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (64, 4)
mat2_sym_strides:        (1, 64)"]
	140649815543280 -> 140649815543088
	140649816396304 [label="block_1_0.cbam.channel_attention.shared_mlp.1.bias
 (4)" fillcolor=lightblue]
	140649816396304 -> 140649815543280
	140649815543280 [label=AccumulateGrad]
	140649815543232 -> 140649815543088
	140649815543232 [label="ReshapeAliasBackward0
-----------------------------
self_sym_sizes: (1, 64, 1, 1)"]
	140649815543376 -> 140649815543232
	140649815543376 [label="AvgPool2DBackward0
---------------------------------
ceil_mode        :          False
count_include_pad:           True
divisor_override :           None
kernel_size      :       (32, 32)
padding          :         (0, 0)
self             : [saved tensor]
stride           :       (32, 32)"]
	140649815540208 -> 140649815543376
	140649815543184 -> 140649815543088
	140649815543184 [label=TBackward0]
	140649815543520 -> 140649815543184
	140649816396224 [label="block_1_0.cbam.channel_attention.shared_mlp.1.weight
 (4, 64)" fillcolor=lightblue]
	140649816396224 -> 140649815543520
	140649815543520 [label=AccumulateGrad]
	140649815542896 -> 140649815542848
	140649815542896 [label=TBackward0]
	140649815543568 -> 140649815542896
	140649816396384 [label="block_1_0.cbam.channel_attention.shared_mlp.3.weight
 (64, 4)" fillcolor=lightblue]
	140649816396384 -> 140649815543568
	140649815543568 [label=AccumulateGrad]
	140649815542608 -> 140649815542656
	140649815542608 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :         (1, 4)
mat1_sym_strides:         (4, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (4, 64)
mat2_sym_strides:         (1, 4)"]
	140649815542992 -> 140649815542608
	140649815543472 -> 140649815542608
	140649815543472 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649815543424 -> 140649815543472
	140649815543424 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :        (1, 64)
mat1_sym_strides:        (64, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (64, 4)
mat2_sym_strides:        (1, 64)"]
	140649815543280 -> 140649815543424
	140649815543712 -> 140649815543424
	140649815543712 [label="ReshapeAliasBackward0
-----------------------------
self_sym_sizes: (1, 64, 1, 1)"]
	140649815543856 -> 140649815543712
	140649815543856 [label="MaxPool2DWithIndicesBackward0
-----------------------------
ceil_mode  :          False
dilation   :         (1, 1)
kernel_size:       (32, 32)
padding    :         (0, 0)
result1    : [saved tensor]
self       : [saved tensor]
stride     :       (32, 32)"]
	140649815540208 -> 140649815543856
	140649815543664 -> 140649815543424
	140649815543664 [label=TBackward0]
	140649815543520 -> 140649815543664
	140649815543328 -> 140649815542608
	140649815543328 [label=TBackward0]
	140649815543568 -> 140649815543328
	140649815540016 -> 140649815539920
	140649815540016 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649815540544 -> 140649815540016
	140649815540544 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815542272 -> 140649815540544
	140649815542272 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (3, 3)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815543904 -> 140649815542272
	140649815543904 [label="CatBackward0
------------
dim: 1"]
	140649815543952 -> 140649815543904
	140649815543952 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649815544000 -> 140649815543952
	140649815544000 [label="MaxBackward0
---------------------------
dim       :               1
indices   :  [saved tensor]
keepdim   :           False
self_sizes: (1, 64, 32, 32)"]
	140649815540064 -> 140649815544000
	140649815543136 -> 140649815543904
	140649815543136 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649815544048 -> 140649815543136
	140649815544048 [label="MeanBackward1
-------------------------------
dim           :            (1,)
keepdim       :           False
self          :  [saved tensor]
self_sym_sizes: (1, 64, 32, 32)"]
	140649815540064 -> 140649815544048
	140649815543040 -> 140649815542272
	140649816396704 [label="block_1_0.cbam.spatial_attention.spatial_attention.0.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	140649816396704 -> 140649815543040
	140649815543040 [label=AccumulateGrad]
	140649815540736 -> 140649815540544
	140649816396624 [label="block_1_0.cbam.spatial_attention.spatial_attention.1.weight
 (1)" fillcolor=lightblue]
	140649816396624 -> 140649815540736
	140649815540736 [label=AccumulateGrad]
	140649815540112 -> 140649815540544
	140649816396784 [label="block_1_0.cbam.spatial_attention.spatial_attention.1.bias
 (1)" fillcolor=lightblue]
	140649816396784 -> 140649815540112
	140649815540112 [label=AccumulateGrad]
	140649815539872 -> 140649815456448
	140649815457744 -> 140649815457696
	140649815457744 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815541024 -> 140649815457744
	140649815541024 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815544144 -> 140649815541024
	140649815544144 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815543760 -> 140649815544144
	140649815543760 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             64
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815456448 -> 140649815543760
	140649815544288 -> 140649815543760
	140649816397264 [label="block_1_1.conv_0.depthwise.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	140649816397264 -> 140649815544288
	140649815544288 [label=AccumulateGrad]
	140649815543808 -> 140649815544144
	140649816397424 [label="block_1_1.conv_0.pointwise.weight
 (32, 64, 1, 1)" fillcolor=lightblue]
	140649816397424 -> 140649815543808
	140649815543808 [label=AccumulateGrad]
	140649815540256 -> 140649815541024
	140649816398464 [label="block_1_1.bn_0.weight
 (32)" fillcolor=lightblue]
	140649816398464 -> 140649815540256
	140649815540256 [label=AccumulateGrad]
	140649815542800 -> 140649815541024
	140649816398544 [label="block_1_1.bn_0.bias
 (32)" fillcolor=lightblue]
	140649816398544 -> 140649815542800
	140649815542800 [label=AccumulateGrad]
	140649815539968 -> 140649815457744
	140649816400384 [label="block_1_1.actv_0.weight
 (32)" fillcolor=lightblue]
	140649816400384 -> 140649815539968
	140649815539968 [label=AccumulateGrad]
	140649815457648 -> 140649815457552
	140649815457648 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815544336 -> 140649815457648
	140649815544336 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815544384 -> 140649815544336
	140649815544384 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815544480 -> 140649815544384
	140649815544480 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             96
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815457696 -> 140649815544480
	140649815544624 -> 140649815544480
	140649816397584 [label="block_1_1.conv_1.depthwise.weight
 (96, 1, 3, 3)" fillcolor=lightblue]
	140649816397584 -> 140649815544624
	140649815544624 [label=AccumulateGrad]
	140649815544432 -> 140649815544384
	140649816397744 [label="block_1_1.conv_1.pointwise.weight
 (32, 96, 1, 1)" fillcolor=lightblue]
	140649816397744 -> 140649815544432
	140649815544432 [label=AccumulateGrad]
	140649815544096 -> 140649815544336
	140649816398944 [label="block_1_1.bn_1.weight
 (32)" fillcolor=lightblue]
	140649816398944 -> 140649815544096
	140649815544096 [label=AccumulateGrad]
	140649815543616 -> 140649815544336
	140649816399024 [label="block_1_1.bn_1.bias
 (32)" fillcolor=lightblue]
	140649816399024 -> 140649815543616
	140649815543616 [label=AccumulateGrad]
	140649815539776 -> 140649815457648
	140649816400464 [label="block_1_1.actv_1.weight
 (32)" fillcolor=lightblue]
	140649816400464 -> 140649815539776
	140649815539776 [label=AccumulateGrad]
	140649815457504 -> 140649815457408
	140649815457504 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815457600 -> 140649815457504
	140649815457600 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815544720 -> 140649815457600
	140649815544720 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815544816 -> 140649815544720
	140649815544816 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            128
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815457552 -> 140649815544816
	140649815544960 -> 140649815544816
	140649816397904 [label="block_1_1.conv_2.depthwise.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	140649816397904 -> 140649815544960
	140649815544960 [label=AccumulateGrad]
	140649815544768 -> 140649815544720
	140649816398064 [label="block_1_1.conv_2.pointwise.weight
 (32, 128, 1, 1)" fillcolor=lightblue]
	140649816398064 -> 140649815544768
	140649815544768 [label=AccumulateGrad]
	140649815544240 -> 140649815457600
	140649816399424 [label="block_1_1.bn_2.weight
 (32)" fillcolor=lightblue]
	140649816399424 -> 140649815544240
	140649815544240 [label=AccumulateGrad]
	140649815544192 -> 140649815457600
	140649816399504 [label="block_1_1.bn_2.bias
 (32)" fillcolor=lightblue]
	140649816399504 -> 140649815544192
	140649815544192 [label=AccumulateGrad]
	140649815544672 -> 140649815457504
	140649816400544 [label="block_1_1.actv_2.weight
 (32)" fillcolor=lightblue]
	140649816400544 -> 140649815544672
	140649815544672 [label=AccumulateGrad]
	140649815457360 -> 140649815457264
	140649816398224 [label="block_1_1.conv_3.depthwise.weight
 (160, 1, 3, 3)" fillcolor=lightblue]
	140649816398224 -> 140649815457360
	140649815457360 [label=AccumulateGrad]
	140649815457216 -> 140649815457072
	140649816398384 [label="block_1_1.conv_3.pointwise.weight
 (64, 160, 1, 1)" fillcolor=lightblue]
	140649816398384 -> 140649815457216
	140649815457216 [label=AccumulateGrad]
	140649815457024 -> 140649815456928
	140649816399904 [label="block_1_1.bn_3.weight
 (64)" fillcolor=lightblue]
	140649816399904 -> 140649815457024
	140649815457024 [label=AccumulateGrad]
	140649815456976 -> 140649815456928
	140649816399984 [label="block_1_1.bn_3.bias
 (64)" fillcolor=lightblue]
	140649816399984 -> 140649815456976
	140649815456976 [label=AccumulateGrad]
	140649815456880 -> 140649815456784
	140649816400624 [label="block_1_1.actv_3.weight
 (64)" fillcolor=lightblue]
	140649816400624 -> 140649815456880
	140649815456880 [label=AccumulateGrad]
	140649815456736 -> 140649815456640
	140649815456736 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (1, 64, 1, 1)"]
	140649815457168 -> 140649815456736
	140649815457168 [label="UnsqueezeBackward0
------------------
dim: 3"]
	140649815457312 -> 140649815457168
	140649815457312 [label="UnsqueezeBackward0
------------------
dim: 2"]
	140649815456832 -> 140649815457312
	140649815456832 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649815544576 -> 140649815456832
	140649815544576 [label="SumBackward1
--------------------------
dim           :       (0,)
keepdim       :      False
self_sym_sizes: (2, 1, 64)"]
	140649815544912 -> 140649815544576
	140649815544912 [label="StackBackward0
--------------
dim: 0"]
	140649815545104 -> 140649815544912
	140649815545104 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :         (1, 4)
mat1_sym_strides:         (4, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (4, 64)
mat2_sym_strides:         (1, 4)"]
	140649815545248 -> 140649815545104
	140649816400944 [label="block_1_1.cbam.channel_attention.shared_mlp.3.bias
 (64)" fillcolor=lightblue]
	140649816400944 -> 140649815545248
	140649815545248 [label=AccumulateGrad]
	140649815545200 -> 140649815545104
	140649815545200 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649815545344 -> 140649815545200
	140649815545344 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :        (1, 64)
mat1_sym_strides:        (64, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (64, 4)
mat2_sym_strides:        (1, 64)"]
	140649815545536 -> 140649815545344
	140649816400784 [label="block_1_1.cbam.channel_attention.shared_mlp.1.bias
 (4)" fillcolor=lightblue]
	140649816400784 -> 140649815545536
	140649815545536 [label=AccumulateGrad]
	140649815545488 -> 140649815545344
	140649815545488 [label="ReshapeAliasBackward0
-----------------------------
self_sym_sizes: (1, 64, 1, 1)"]
	140649815545632 -> 140649815545488
	140649815545632 [label="AvgPool2DBackward0
---------------------------------
ceil_mode        :          False
count_include_pad:           True
divisor_override :           None
kernel_size      :       (32, 32)
padding          :         (0, 0)
self             : [saved tensor]
stride           :       (32, 32)"]
	140649815456784 -> 140649815545632
	140649815545440 -> 140649815545344
	140649815545440 [label=TBackward0]
	140649815545728 -> 140649815545440
	140649816400704 [label="block_1_1.cbam.channel_attention.shared_mlp.1.weight
 (4, 64)" fillcolor=lightblue]
	140649816400704 -> 140649815545728
	140649815545728 [label=AccumulateGrad]
	140649815545152 -> 140649815545104
	140649815545152 [label=TBackward0]
	140649815545824 -> 140649815545152
	140649816400864 [label="block_1_1.cbam.channel_attention.shared_mlp.3.weight
 (64, 4)" fillcolor=lightblue]
	140649816400864 -> 140649815545824
	140649815545824 [label=AccumulateGrad]
	140649815544864 -> 140649815544912
	140649815544864 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :         (1, 4)
mat1_sym_strides:         (4, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (4, 64)
mat2_sym_strides:         (1, 4)"]
	140649815545248 -> 140649815544864
	140649815545872 -> 140649815544864
	140649815545872 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649815545680 -> 140649815545872
	140649815545680 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :        (1, 64)
mat1_sym_strides:        (64, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (64, 4)
mat2_sym_strides:        (1, 64)"]
	140649815545536 -> 140649815545680
	140649815546016 -> 140649815545680
	140649815546016 [label="ReshapeAliasBackward0
-----------------------------
self_sym_sizes: (1, 64, 1, 1)"]
	140649815546160 -> 140649815546016
	140649815546160 [label="MaxPool2DWithIndicesBackward0
-----------------------------
ceil_mode  :          False
dilation   :         (1, 1)
kernel_size:       (32, 32)
padding    :         (0, 0)
result1    : [saved tensor]
self       : [saved tensor]
stride     :       (32, 32)"]
	140649815456784 -> 140649815546160
	140649815545968 -> 140649815545680
	140649815545968 [label=TBackward0]
	140649815545728 -> 140649815545968
	140649815545584 -> 140649815544864
	140649815545584 [label=TBackward0]
	140649815545824 -> 140649815545584
	140649815456592 -> 140649815456496
	140649815456592 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649815457120 -> 140649815456592
	140649815457120 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815456688 -> 140649815457120
	140649815456688 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (3, 3)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815546208 -> 140649815456688
	140649815546208 [label="CatBackward0
------------
dim: 1"]
	140649815546256 -> 140649815546208
	140649815546256 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649815546304 -> 140649815546256
	140649815546304 [label="MaxBackward0
---------------------------
dim       :               1
indices   :  [saved tensor]
keepdim   :           False
self_sizes: (1, 64, 32, 32)"]
	140649815456640 -> 140649815546304
	140649815545392 -> 140649815546208
	140649815545392 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649815546352 -> 140649815545392
	140649815546352 [label="MeanBackward1
-------------------------------
dim           :            (1,)
keepdim       :           False
self          :  [saved tensor]
self_sym_sizes: (1, 64, 32, 32)"]
	140649815456640 -> 140649815546352
	140649815545296 -> 140649815456688
	140649816401184 [label="block_1_1.cbam.spatial_attention.spatial_attention.0.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	140649816401184 -> 140649815545296
	140649815545296 [label=AccumulateGrad]
	140649815544528 -> 140649815457120
	140649816401104 [label="block_1_1.cbam.spatial_attention.spatial_attention.1.weight
 (1)" fillcolor=lightblue]
	140649816401104 -> 140649815544528
	140649815544528 [label=AccumulateGrad]
	140649815539824 -> 140649815457120
	140649816401264 [label="block_1_1.cbam.spatial_attention.spatial_attention.1.bias
 (1)" fillcolor=lightblue]
	140649816401264 -> 140649815539824
	140649815539824 [label=AccumulateGrad]
	140649815456448 -> 140649815456352
	140649815456304 -> 140649815456208
	140649815456304 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815457456 -> 140649815456304
	140649815457456 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (2, 2)
transposed        :           True
weight            : [saved tensor]"]
	140649815546448 -> 140649815457456
	140649815546448 [label="AddBackward0
------------
alpha: 1"]
	140649815546064 -> 140649815546448
	140649815546064 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815546592 -> 140649815546064
	140649815546592 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815546736 -> 140649815546592
	140649815546736 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815546880 -> 140649815546736
	140649815546880 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815547024 -> 140649815546880
	140649815547024 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815547216 -> 140649815547024
	140649815547216 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            320
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815547360 -> 140649815547216
	140649815547360 [label="CatBackward0
------------
dim: 1"]
	140649815547504 -> 140649815547360
	140649815547504 [label="CatBackward0
------------
dim: 1"]
	140649815547648 -> 140649815547504
	140649815547648 [label="CatBackward0
------------
dim: 1"]
	140649815546112 -> 140649815547648
	140649815546112 [label="AddBackward0
------------
alpha: 1"]
	140649815547888 -> 140649815546112
	140649815547888 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815548032 -> 140649815547888
	140649815548032 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815548176 -> 140649815548032
	140649815548176 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815548320 -> 140649815548176
	140649815548320 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815548464 -> 140649815548320
	140649815548464 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815548656 -> 140649815548464
	140649815548656 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            320
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815548800 -> 140649815548656
	140649815548800 [label="CatBackward0
------------
dim: 1"]
	140649815548944 -> 140649815548800
	140649815548944 [label="CatBackward0
------------
dim: 1"]
	140649815549088 -> 140649815548944
	140649815549088 [label="CatBackward0
------------
dim: 1"]
	140649815547840 -> 140649815549088
	140649815547840 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815549328 -> 140649815547840
	140649815549328 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815549472 -> 140649815549328
	140649815549472 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            384
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815549616 -> 140649815549472
	140649815549616 [label="CatBackward0
------------
dim: 1"]
	140649815549760 -> 140649815549616
	140649815549760 [label="AddBackward0
------------
alpha: 1"]
	140649815549904 -> 140649815549760
	140649815549904 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815550048 -> 140649815549904
	140649815550048 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815550192 -> 140649815550048
	140649815550192 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815550336 -> 140649815550192
	140649815550336 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815550480 -> 140649815550336
	140649815550480 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815550672 -> 140649815550480
	140649815550672 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            320
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815550816 -> 140649815550672
	140649815550816 [label="CatBackward0
------------
dim: 1"]
	140649815550960 -> 140649815550816
	140649815550960 [label="CatBackward0
------------
dim: 1"]
	140649815551104 -> 140649815550960
	140649815551104 [label="CatBackward0
------------
dim: 1"]
	140649815549856 -> 140649815551104
	140649815549856 [label="AddBackward0
------------
alpha: 1"]
	140649815551344 -> 140649815549856
	140649815551344 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815551488 -> 140649815551344
	140649815551488 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815551632 -> 140649815551488
	140649815551632 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815551776 -> 140649815551632
	140649815551776 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815551920 -> 140649815551776
	140649815551920 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815552112 -> 140649815551920
	140649815552112 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            320
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815552256 -> 140649815552112
	140649815552256 [label="CatBackward0
------------
dim: 1"]
	140649815552400 -> 140649815552256
	140649815552400 [label="CatBackward0
------------
dim: 1"]
	140649815552544 -> 140649815552400
	140649815552544 [label="CatBackward0
------------
dim: 1"]
	140649815551296 -> 140649815552544
	140649815551296 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815552784 -> 140649815551296
	140649815552784 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	140649815456352 -> 140649815552784
	140649815552928 -> 140649815552784
	140649816401664 [label="down_1.conv.weight
 (128, 64, 2, 2)" fillcolor=lightblue]
	140649816401664 -> 140649815552928
	140649815552928 [label=AccumulateGrad]
	140649815552880 -> 140649815552784
	140649816401744 [label="down_1.conv.bias
 (128)" fillcolor=lightblue]
	140649816401744 -> 140649815552880
	140649815552880 [label=AccumulateGrad]
	140649815552736 -> 140649815551296
	140649816401824 [label="down_1.actv.weight
 (128)" fillcolor=lightblue]
	140649816401824 -> 140649815552736
	140649815552736 [label=AccumulateGrad]
	140649815552688 -> 140649815552544
	140649815552688 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815552976 -> 140649815552688
	140649815552976 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815553120 -> 140649815552976
	140649815553120 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815553312 -> 140649815553120
	140649815553312 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            128
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815551296 -> 140649815553312
	140649815553456 -> 140649815553312
	140649816401984 [label="block_2_0.conv_0.depthwise.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	140649816401984 -> 140649815553456
	140649815553456 [label=AccumulateGrad]
	140649815553264 -> 140649815553120
	140649816402144 [label="block_2_0.conv_0.pointwise.weight
 (64, 128, 1, 1)" fillcolor=lightblue]
	140649816402144 -> 140649815553264
	140649815553264 [label=AccumulateGrad]
	140649815553072 -> 140649815552976
	140649816403184 [label="block_2_0.bn_0.weight
 (64)" fillcolor=lightblue]
	140649816403184 -> 140649815553072
	140649815553072 [label=AccumulateGrad]
	140649815552832 -> 140649815552976
	140649816403264 [label="block_2_0.bn_0.bias
 (64)" fillcolor=lightblue]
	140649816403264 -> 140649815552832
	140649815552832 [label=AccumulateGrad]
	140649815553024 -> 140649815552688
	140649816405104 [label="block_2_0.actv_0.weight
 (64)" fillcolor=lightblue]
	140649816405104 -> 140649815553024
	140649815553024 [label=AccumulateGrad]
	140649815552496 -> 140649815552400
	140649815552496 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815553504 -> 140649815552496
	140649815553504 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815553552 -> 140649815553504
	140649815553552 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815553648 -> 140649815553552
	140649815553648 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            192
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815552544 -> 140649815553648
	140649815553792 -> 140649815553648
	140649816402304 [label="block_2_0.conv_1.depthwise.weight
 (192, 1, 3, 3)" fillcolor=lightblue]
	140649816402304 -> 140649815553792
	140649815553792 [label=AccumulateGrad]
	140649815553600 -> 140649815553552
	140649816402464 [label="block_2_0.conv_1.pointwise.weight
 (64, 192, 1, 1)" fillcolor=lightblue]
	140649816402464 -> 140649815553600
	140649815553600 [label=AccumulateGrad]
	140649815553168 -> 140649815553504
	140649816403664 [label="block_2_0.bn_1.weight
 (64)" fillcolor=lightblue]
	140649816403664 -> 140649815553168
	140649815553168 [label=AccumulateGrad]
	140649815553216 -> 140649815553504
	140649816403744 [label="block_2_0.bn_1.bias
 (64)" fillcolor=lightblue]
	140649816403744 -> 140649815553216
	140649815553216 [label=AccumulateGrad]
	140649815552592 -> 140649815552496
	140649816405184 [label="block_2_0.actv_1.weight
 (64)" fillcolor=lightblue]
	140649816405184 -> 140649815552592
	140649815552592 [label=AccumulateGrad]
	140649815552352 -> 140649815552256
	140649815552352 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815553840 -> 140649815552352
	140649815553840 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815553888 -> 140649815553840
	140649815553888 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815553984 -> 140649815553888
	140649815553984 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            256
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815552400 -> 140649815553984
	140649815554128 -> 140649815553984
	140649816402624 [label="block_2_0.conv_2.depthwise.weight
 (256, 1, 3, 3)" fillcolor=lightblue]
	140649816402624 -> 140649815554128
	140649815554128 [label=AccumulateGrad]
	140649815553936 -> 140649815553888
	140649816402784 [label="block_2_0.conv_2.pointwise.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	140649816402784 -> 140649815553936
	140649815553936 [label=AccumulateGrad]
	140649815553408 -> 140649815553840
	140649816404144 [label="block_2_0.bn_2.weight
 (64)" fillcolor=lightblue]
	140649816404144 -> 140649815553408
	140649815553408 [label=AccumulateGrad]
	140649815553360 -> 140649815553840
	140649816404224 [label="block_2_0.bn_2.bias
 (64)" fillcolor=lightblue]
	140649816404224 -> 140649815553360
	140649815553360 [label=AccumulateGrad]
	140649815552640 -> 140649815552352
	140649816405264 [label="block_2_0.actv_2.weight
 (64)" fillcolor=lightblue]
	140649816405264 -> 140649815552640
	140649815552640 [label=AccumulateGrad]
	140649815552208 -> 140649815552112
	140649816402944 [label="block_2_0.conv_3.depthwise.weight
 (320, 1, 3, 3)" fillcolor=lightblue]
	140649816402944 -> 140649815552208
	140649815552208 [label=AccumulateGrad]
	140649815552064 -> 140649815551920
	140649816403104 [label="block_2_0.conv_3.pointwise.weight
 (128, 320, 1, 1)" fillcolor=lightblue]
	140649816403104 -> 140649815552064
	140649815552064 [label=AccumulateGrad]
	140649815551872 -> 140649815551776
	140649816404624 [label="block_2_0.bn_3.weight
 (128)" fillcolor=lightblue]
	140649816404624 -> 140649815551872
	140649815551872 [label=AccumulateGrad]
	140649815551824 -> 140649815551776
	140649816404704 [label="block_2_0.bn_3.bias
 (128)" fillcolor=lightblue]
	140649816404704 -> 140649815551824
	140649815551824 [label=AccumulateGrad]
	140649815551728 -> 140649815551632
	140649816405344 [label="block_2_0.actv_3.weight
 (128)" fillcolor=lightblue]
	140649816405344 -> 140649815551728
	140649815551728 [label=AccumulateGrad]
	140649815551584 -> 140649815551488
	140649815551584 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 128, 1, 1)"]
	140649815552016 -> 140649815551584
	140649815552016 [label="UnsqueezeBackward0
------------------
dim: 3"]
	140649815554176 -> 140649815552016
	140649815554176 [label="UnsqueezeBackward0
------------------
dim: 2"]
	140649815552304 -> 140649815554176
	140649815552304 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649815553744 -> 140649815552304
	140649815553744 [label="SumBackward1
---------------------------
dim           :        (0,)
keepdim       :       False
self_sym_sizes: (2, 1, 128)"]
	140649815554080 -> 140649815553744
	140649815554080 [label="StackBackward0
--------------
dim: 0"]
	140649815554272 -> 140649815554080
	140649815554272 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :         (1, 8)
mat1_sym_strides:         (8, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :       (8, 128)
mat2_sym_strides:         (1, 8)"]
	140649815554416 -> 140649815554272
	140649816405664 [label="block_2_0.cbam.channel_attention.shared_mlp.3.bias
 (128)" fillcolor=lightblue]
	140649816405664 -> 140649815554416
	140649815554416 [label=AccumulateGrad]
	140649815554368 -> 140649815554272
	140649815554368 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649815554512 -> 140649815554368
	140649815554512 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 128)
mat1_sym_strides:       (128, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :       (128, 8)
mat2_sym_strides:       (1, 128)"]
	140649815554704 -> 140649815554512
	140649816405504 [label="block_2_0.cbam.channel_attention.shared_mlp.1.bias
 (8)" fillcolor=lightblue]
	140649816405504 -> 140649815554704
	140649815554704 [label=AccumulateGrad]
	140649815554656 -> 140649815554512
	140649815554656 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (1, 128, 1, 1)"]
	140649815554800 -> 140649815554656
	140649815554800 [label="AvgPool2DBackward0
---------------------------------
ceil_mode        :          False
count_include_pad:           True
divisor_override :           None
kernel_size      :       (16, 16)
padding          :         (0, 0)
self             : [saved tensor]
stride           :       (16, 16)"]
	140649815551632 -> 140649815554800
	140649815554608 -> 140649815554512
	140649815554608 [label=TBackward0]
	140649815554944 -> 140649815554608
	140649816405424 [label="block_2_0.cbam.channel_attention.shared_mlp.1.weight
 (8, 128)" fillcolor=lightblue]
	140649816405424 -> 140649815554944
	140649815554944 [label=AccumulateGrad]
	140649815554320 -> 140649815554272
	140649815554320 [label=TBackward0]
	140649815554992 -> 140649815554320
	140649816405584 [label="block_2_0.cbam.channel_attention.shared_mlp.3.weight
 (128, 8)" fillcolor=lightblue]
	140649816405584 -> 140649815554992
	140649815554992 [label=AccumulateGrad]
	140649815554032 -> 140649815554080
	140649815554032 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :         (1, 8)
mat1_sym_strides:         (8, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :       (8, 128)
mat2_sym_strides:         (1, 8)"]
	140649815554416 -> 140649815554032
	140649815554896 -> 140649815554032
	140649815554896 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649815554848 -> 140649815554896
	140649815554848 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 128)
mat1_sym_strides:       (128, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :       (128, 8)
mat2_sym_strides:       (1, 128)"]
	140649815554704 -> 140649815554848
	140649815555136 -> 140649815554848
	140649815555136 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (1, 128, 1, 1)"]
	140649815555280 -> 140649815555136
	140649815555280 [label="MaxPool2DWithIndicesBackward0
-----------------------------
ceil_mode  :          False
dilation   :         (1, 1)
kernel_size:       (16, 16)
padding    :         (0, 0)
result1    : [saved tensor]
self       : [saved tensor]
stride     :       (16, 16)"]
	140649815551632 -> 140649815555280
	140649815555088 -> 140649815554848
	140649815555088 [label=TBackward0]
	140649815554944 -> 140649815555088
	140649815554752 -> 140649815554032
	140649815554752 [label=TBackward0]
	140649815554992 -> 140649815554752
	140649815551440 -> 140649815551344
	140649815551440 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649815551968 -> 140649815551440
	140649815551968 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815553696 -> 140649815551968
	140649815553696 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (3, 3)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815555328 -> 140649815553696
	140649815555328 [label="CatBackward0
------------
dim: 1"]
	140649815555376 -> 140649815555328
	140649815555376 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649815555424 -> 140649815555376
	140649815555424 [label="MaxBackward0
----------------------------
dim       :                1
indices   :   [saved tensor]
keepdim   :            False
self_sizes: (1, 128, 16, 16)"]
	140649815551488 -> 140649815555424
	140649815554560 -> 140649815555328
	140649815554560 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649815555472 -> 140649815554560
	140649815555472 [label="MeanBackward1
--------------------------------
dim           :             (1,)
keepdim       :            False
self          :   [saved tensor]
self_sym_sizes: (1, 128, 16, 16)"]
	140649815551488 -> 140649815555472
	140649815554464 -> 140649815553696
	140649816405904 [label="block_2_0.cbam.spatial_attention.spatial_attention.0.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	140649816405904 -> 140649815554464
	140649815554464 [label=AccumulateGrad]
	140649815552160 -> 140649815551968
	140649816405824 [label="block_2_0.cbam.spatial_attention.spatial_attention.1.weight
 (1)" fillcolor=lightblue]
	140649816405824 -> 140649815552160
	140649815552160 [label=AccumulateGrad]
	140649815551536 -> 140649815551968
	140649816405984 [label="block_2_0.cbam.spatial_attention.spatial_attention.1.bias
 (1)" fillcolor=lightblue]
	140649816405984 -> 140649815551536
	140649815551536 [label=AccumulateGrad]
	140649815551296 -> 140649815549856
	140649815551248 -> 140649815551104
	140649815551248 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815552448 -> 140649815551248
	140649815552448 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815555568 -> 140649815552448
	140649815555568 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815555184 -> 140649815555568
	140649815555184 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            128
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815549856 -> 140649815555184
	140649815555712 -> 140649815555184
	140649816406464 [label="block_2_1.conv_0.depthwise.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	140649816406464 -> 140649815555712
	140649815555712 [label=AccumulateGrad]
	140649815555232 -> 140649815555568
	140649816406624 [label="block_2_1.conv_0.pointwise.weight
 (64, 128, 1, 1)" fillcolor=lightblue]
	140649816406624 -> 140649815555232
	140649815555232 [label=AccumulateGrad]
	140649815551680 -> 140649815552448
	140649816407664 [label="block_2_1.bn_0.weight
 (64)" fillcolor=lightblue]
	140649816407664 -> 140649815551680
	140649815551680 [label=AccumulateGrad]
	140649815554224 -> 140649815552448
	140649816407744 [label="block_2_1.bn_0.bias
 (64)" fillcolor=lightblue]
	140649816407744 -> 140649815554224
	140649815554224 [label=AccumulateGrad]
	140649815551392 -> 140649815551248
	140649816688176 [label="block_2_1.actv_0.weight
 (64)" fillcolor=lightblue]
	140649816688176 -> 140649815551392
	140649815551392 [label=AccumulateGrad]
	140649815551056 -> 140649815550960
	140649815551056 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815555760 -> 140649815551056
	140649815555760 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815555808 -> 140649815555760
	140649815555808 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815555904 -> 140649815555808
	140649815555904 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            192
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815551104 -> 140649815555904
	140649815556048 -> 140649815555904
	140649816406784 [label="block_2_1.conv_1.depthwise.weight
 (192, 1, 3, 3)" fillcolor=lightblue]
	140649816406784 -> 140649815556048
	140649815556048 [label=AccumulateGrad]
	140649815555856 -> 140649815555808
	140649816406944 [label="block_2_1.conv_1.pointwise.weight
 (64, 192, 1, 1)" fillcolor=lightblue]
	140649816406944 -> 140649815555856
	140649815555856 [label=AccumulateGrad]
	140649815555520 -> 140649815555760
	140649816686736 [label="block_2_1.bn_1.weight
 (64)" fillcolor=lightblue]
	140649816686736 -> 140649815555520
	140649815555520 [label=AccumulateGrad]
	140649815555040 -> 140649815555760
	140649816686816 [label="block_2_1.bn_1.bias
 (64)" fillcolor=lightblue]
	140649816686816 -> 140649815555040
	140649815555040 [label=AccumulateGrad]
	140649815551152 -> 140649815551056
	140649816688256 [label="block_2_1.actv_1.weight
 (64)" fillcolor=lightblue]
	140649816688256 -> 140649815551152
	140649815551152 [label=AccumulateGrad]
	140649815550912 -> 140649815550816
	140649815550912 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815555952 -> 140649815550912
	140649815555952 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815556000 -> 140649815555952
	140649815556000 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815687376 -> 140649815556000
	140649815687376 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            256
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815550960 -> 140649815687376
	140649815687520 -> 140649815687376
	140649816407104 [label="block_2_1.conv_2.depthwise.weight
 (256, 1, 3, 3)" fillcolor=lightblue]
	140649816407104 -> 140649815687520
	140649815687520 [label=AccumulateGrad]
	140649815687328 -> 140649815556000
	140649816407264 [label="block_2_1.conv_2.pointwise.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	140649816407264 -> 140649815687328
	140649815687328 [label=AccumulateGrad]
	140649815555664 -> 140649815555952
	140649816687216 [label="block_2_1.bn_2.weight
 (64)" fillcolor=lightblue]
	140649816687216 -> 140649815555664
	140649815555664 [label=AccumulateGrad]
	140649815555616 -> 140649815555952
	140649816687296 [label="block_2_1.bn_2.bias
 (64)" fillcolor=lightblue]
	140649816687296 -> 140649815555616
	140649815555616 [label=AccumulateGrad]
	140649815551200 -> 140649815550912
	140649816688336 [label="block_2_1.actv_2.weight
 (64)" fillcolor=lightblue]
	140649816688336 -> 140649815551200
	140649815551200 [label=AccumulateGrad]
	140649815550768 -> 140649815550672
	140649816407424 [label="block_2_1.conv_3.depthwise.weight
 (320, 1, 3, 3)" fillcolor=lightblue]
	140649816407424 -> 140649815550768
	140649815550768 [label=AccumulateGrad]
	140649815550624 -> 140649815550480
	140649816407584 [label="block_2_1.conv_3.pointwise.weight
 (128, 320, 1, 1)" fillcolor=lightblue]
	140649816407584 -> 140649815550624
	140649815550624 [label=AccumulateGrad]
	140649815550432 -> 140649815550336
	140649816687696 [label="block_2_1.bn_3.weight
 (128)" fillcolor=lightblue]
	140649816687696 -> 140649815550432
	140649815550432 [label=AccumulateGrad]
	140649815550384 -> 140649815550336
	140649816687776 [label="block_2_1.bn_3.bias
 (128)" fillcolor=lightblue]
	140649816687776 -> 140649815550384
	140649815550384 [label=AccumulateGrad]
	140649815550288 -> 140649815550192
	140649816688416 [label="block_2_1.actv_3.weight
 (128)" fillcolor=lightblue]
	140649816688416 -> 140649815550288
	140649815550288 [label=AccumulateGrad]
	140649815550144 -> 140649815550048
	140649815550144 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 128, 1, 1)"]
	140649815550576 -> 140649815550144
	140649815550576 [label="UnsqueezeBackward0
------------------
dim: 3"]
	140649815551008 -> 140649815550576
	140649815551008 [label="UnsqueezeBackward0
------------------
dim: 2"]
	140649815550720 -> 140649815551008
	140649815550720 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649815687280 -> 140649815550720
	140649815687280 [label="SumBackward1
---------------------------
dim           :        (0,)
keepdim       :       False
self_sym_sizes: (2, 1, 128)"]
	140649815687472 -> 140649815687280
	140649815687472 [label="StackBackward0
--------------
dim: 0"]
	140649815687664 -> 140649815687472
	140649815687664 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :         (1, 8)
mat1_sym_strides:         (8, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :       (8, 128)
mat2_sym_strides:         (1, 8)"]
	140649815687808 -> 140649815687664
	140649816688656 [label="block_2_1.cbam.channel_attention.shared_mlp.3.bias
 (128)" fillcolor=lightblue]
	140649816688656 -> 140649815687808
	140649815687808 [label=AccumulateGrad]
	140649815687760 -> 140649815687664
	140649815687760 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649815687904 -> 140649815687760
	140649815687904 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 128)
mat1_sym_strides:       (128, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :       (128, 8)
mat2_sym_strides:       (1, 128)"]
	140649815688096 -> 140649815687904
	140649816688496 [label="block_2_1.cbam.channel_attention.shared_mlp.1.bias
 (8)" fillcolor=lightblue]
	140649816688496 -> 140649815688096
	140649815688096 [label=AccumulateGrad]
	140649815688048 -> 140649815687904
	140649815688048 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (1, 128, 1, 1)"]
	140649815688192 -> 140649815688048
	140649815688192 [label="AvgPool2DBackward0
---------------------------------
ceil_mode        :          False
count_include_pad:           True
divisor_override :           None
kernel_size      :       (16, 16)
padding          :         (0, 0)
self             : [saved tensor]
stride           :       (16, 16)"]
	140649815550192 -> 140649815688192
	140649815688000 -> 140649815687904
	140649815688000 [label=TBackward0]
	140649815688336 -> 140649815688000
	140649816688096 [label="block_2_1.cbam.channel_attention.shared_mlp.1.weight
 (8, 128)" fillcolor=lightblue]
	140649816688096 -> 140649815688336
	140649815688336 [label=AccumulateGrad]
	140649815687712 -> 140649815687664
	140649815687712 [label=TBackward0]
	140649815688384 -> 140649815687712
	140649816688576 [label="block_2_1.cbam.channel_attention.shared_mlp.3.weight
 (128, 8)" fillcolor=lightblue]
	140649816688576 -> 140649815688384
	140649815688384 [label=AccumulateGrad]
	140649815687424 -> 140649815687472
	140649815687424 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :         (1, 8)
mat1_sym_strides:         (8, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :       (8, 128)
mat2_sym_strides:         (1, 8)"]
	140649815687808 -> 140649815687424
	140649815688288 -> 140649815687424
	140649815688288 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649815688240 -> 140649815688288
	140649815688240 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 128)
mat1_sym_strides:       (128, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :       (128, 8)
mat2_sym_strides:       (1, 128)"]
	140649815688096 -> 140649815688240
	140649815688528 -> 140649815688240
	140649815688528 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (1, 128, 1, 1)"]
	140649815688672 -> 140649815688528
	140649815688672 [label="MaxPool2DWithIndicesBackward0
-----------------------------
ceil_mode  :          False
dilation   :         (1, 1)
kernel_size:       (16, 16)
padding    :         (0, 0)
result1    : [saved tensor]
self       : [saved tensor]
stride     :       (16, 16)"]
	140649815550192 -> 140649815688672
	140649815688480 -> 140649815688240
	140649815688480 [label=TBackward0]
	140649815688336 -> 140649815688480
	140649815688144 -> 140649815687424
	140649815688144 [label=TBackward0]
	140649815688384 -> 140649815688144
	140649815550000 -> 140649815549904
	140649815550000 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649815550528 -> 140649815550000
	140649815550528 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815550240 -> 140649815550528
	140649815550240 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (3, 3)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815688720 -> 140649815550240
	140649815688720 [label="CatBackward0
------------
dim: 1"]
	140649815688768 -> 140649815688720
	140649815688768 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649815688816 -> 140649815688768
	140649815688816 [label="MaxBackward0
----------------------------
dim       :                1
indices   :   [saved tensor]
keepdim   :            False
self_sizes: (1, 128, 16, 16)"]
	140649815550048 -> 140649815688816
	140649815687952 -> 140649815688720
	140649815687952 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649815688864 -> 140649815687952
	140649815688864 [label="MeanBackward1
--------------------------------
dim           :             (1,)
keepdim       :            False
self          :   [saved tensor]
self_sym_sizes: (1, 128, 16, 16)"]
	140649815550048 -> 140649815688864
	140649815687856 -> 140649815550240
	140649816688896 [label="block_2_1.cbam.spatial_attention.spatial_attention.0.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	140649816688896 -> 140649815687856
	140649815687856 [label=AccumulateGrad]
	140649815550096 -> 140649815550528
	140649816688816 [label="block_2_1.cbam.spatial_attention.spatial_attention.1.weight
 (1)" fillcolor=lightblue]
	140649816688816 -> 140649815550096
	140649815550096 [label=AccumulateGrad]
	140649815687232 -> 140649815550528
	140649816688976 [label="block_2_1.cbam.spatial_attention.spatial_attention.1.bias
 (1)" fillcolor=lightblue]
	140649816688976 -> 140649815687232
	140649815687232 [label=AccumulateGrad]
	140649815549856 -> 140649815549760
	140649815549712 -> 140649815549616
	140649815549712 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815550864 -> 140649815549712
	140649815550864 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (2, 2)
transposed        :           True
weight            : [saved tensor]"]
	140649815688960 -> 140649815550864
	140649815688960 [label="AddBackward0
------------
alpha: 1"]
	140649815688576 -> 140649815688960
	140649815688576 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815689104 -> 140649815688576
	140649815689104 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815689248 -> 140649815689104
	140649815689248 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815689392 -> 140649815689248
	140649815689392 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815689536 -> 140649815689392
	140649815689536 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815689728 -> 140649815689536
	140649815689728 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            640
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815689872 -> 140649815689728
	140649815689872 [label="CatBackward0
------------
dim: 1"]
	140649815690016 -> 140649815689872
	140649815690016 [label="CatBackward0
------------
dim: 1"]
	140649815690160 -> 140649815690016
	140649815690160 [label="CatBackward0
------------
dim: 1"]
	140649815688624 -> 140649815690160
	140649815688624 [label="AddBackward0
------------
alpha: 1"]
	140649815690400 -> 140649815688624
	140649815690400 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815690544 -> 140649815690400
	140649815690544 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140649815690688 -> 140649815690544
	140649815690688 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815690832 -> 140649815690688
	140649815690832 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815690976 -> 140649815690832
	140649815690976 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815691168 -> 140649815690976
	140649815691168 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            640
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815691312 -> 140649815691168
	140649815691312 [label="CatBackward0
------------
dim: 1"]
	140649815691456 -> 140649815691312
	140649815691456 [label="CatBackward0
------------
dim: 1"]
	140649815691600 -> 140649815691456
	140649815691600 [label="CatBackward0
------------
dim: 1"]
	140649815690352 -> 140649815691600
	140649815690352 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815691840 -> 140649815690352
	140649815691840 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	140649815549760 -> 140649815691840
	140649815691984 -> 140649815691840
	140649816689456 [label="down_2.conv.weight
 (256, 128, 2, 2)" fillcolor=lightblue]
	140649816689456 -> 140649815691984
	140649815691984 [label=AccumulateGrad]
	140649815691936 -> 140649815691840
	140649816689536 [label="down_2.conv.bias
 (256)" fillcolor=lightblue]
	140649816689536 -> 140649815691936
	140649815691936 [label=AccumulateGrad]
	140649815691792 -> 140649815690352
	140649816689616 [label="down_2.actv.weight
 (256)" fillcolor=lightblue]
	140649816689616 -> 140649815691792
	140649815691792 [label=AccumulateGrad]
	140649815691744 -> 140649815691600
	140649815691744 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815692032 -> 140649815691744
	140649815692032 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815692176 -> 140649815692032
	140649815692176 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815692368 -> 140649815692176
	140649815692368 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            256
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815690352 -> 140649815692368
	140649815692512 -> 140649815692368
	140649816689776 [label="block_3_0.conv_0.depthwise.weight
 (256, 1, 3, 3)" fillcolor=lightblue]
	140649816689776 -> 140649815692512
	140649815692512 [label=AccumulateGrad]
	140649815692320 -> 140649815692176
	140649816689936 [label="block_3_0.conv_0.pointwise.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	140649816689936 -> 140649815692320
	140649815692320 [label=AccumulateGrad]
	140649815692128 -> 140649815692032
	140649816690976 [label="block_3_0.bn_0.weight
 (128)" fillcolor=lightblue]
	140649816690976 -> 140649815692128
	140649815692128 [label=AccumulateGrad]
	140649815691888 -> 140649815692032
	140649816691056 [label="block_3_0.bn_0.bias
 (128)" fillcolor=lightblue]
	140649816691056 -> 140649815691888
	140649815691888 [label=AccumulateGrad]
	140649815692080 -> 140649815691744
	140649816692896 [label="block_3_0.actv_0.weight
 (128)" fillcolor=lightblue]
	140649816692896 -> 140649815692080
	140649815692080 [label=AccumulateGrad]
	140649815691552 -> 140649815691456
	140649815691552 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815692560 -> 140649815691552
	140649815692560 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815692608 -> 140649815692560
	140649815692608 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815692704 -> 140649815692608
	140649815692704 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            384
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815691600 -> 140649815692704
	140649815692848 -> 140649815692704
	140649816690096 [label="block_3_0.conv_1.depthwise.weight
 (384, 1, 3, 3)" fillcolor=lightblue]
	140649816690096 -> 140649815692848
	140649815692848 [label=AccumulateGrad]
	140649815692656 -> 140649815692608
	140649816690256 [label="block_3_0.conv_1.pointwise.weight
 (128, 384, 1, 1)" fillcolor=lightblue]
	140649816690256 -> 140649815692656
	140649815692656 [label=AccumulateGrad]
	140649815692224 -> 140649815692560
	140649816691456 [label="block_3_0.bn_1.weight
 (128)" fillcolor=lightblue]
	140649816691456 -> 140649815692224
	140649815692224 [label=AccumulateGrad]
	140649815692272 -> 140649815692560
	140649816691536 [label="block_3_0.bn_1.bias
 (128)" fillcolor=lightblue]
	140649816691536 -> 140649815692272
	140649815692272 [label=AccumulateGrad]
	140649815691648 -> 140649815691552
	140649816692976 [label="block_3_0.actv_1.weight
 (128)" fillcolor=lightblue]
	140649816692976 -> 140649815691648
	140649815691648 [label=AccumulateGrad]
	140649815691408 -> 140649815691312
	140649815691408 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815692896 -> 140649815691408
	140649815692896 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815692944 -> 140649815692896
	140649815692944 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815693040 -> 140649815692944
	140649815693040 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            512
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815691456 -> 140649815693040
	140649815693184 -> 140649815693040
	140649816690416 [label="block_3_0.conv_2.depthwise.weight
 (512, 1, 3, 3)" fillcolor=lightblue]
	140649816690416 -> 140649815693184
	140649815693184 [label=AccumulateGrad]
	140649815692992 -> 140649815692944
	140649816690576 [label="block_3_0.conv_2.pointwise.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	140649816690576 -> 140649815692992
	140649815692992 [label=AccumulateGrad]
	140649815692464 -> 140649815692896
	140649816691936 [label="block_3_0.bn_2.weight
 (128)" fillcolor=lightblue]
	140649816691936 -> 140649815692464
	140649815692464 [label=AccumulateGrad]
	140649815692416 -> 140649815692896
	140649816692016 [label="block_3_0.bn_2.bias
 (128)" fillcolor=lightblue]
	140649816692016 -> 140649815692416
	140649815692416 [label=AccumulateGrad]
	140649815691696 -> 140649815691408
	140649816693056 [label="block_3_0.actv_2.weight
 (128)" fillcolor=lightblue]
	140649816693056 -> 140649815691696
	140649815691696 [label=AccumulateGrad]
	140649815691264 -> 140649815691168
	140649816690736 [label="block_3_0.conv_3.depthwise.weight
 (640, 1, 3, 3)" fillcolor=lightblue]
	140649816690736 -> 140649815691264
	140649815691264 [label=AccumulateGrad]
	140649815691120 -> 140649815690976
	140649816690896 [label="block_3_0.conv_3.pointwise.weight
 (256, 640, 1, 1)" fillcolor=lightblue]
	140649816690896 -> 140649815691120
	140649815691120 [label=AccumulateGrad]
	140649815690928 -> 140649815690832
	140649816692416 [label="block_3_0.bn_3.weight
 (256)" fillcolor=lightblue]
	140649816692416 -> 140649815690928
	140649815690928 [label=AccumulateGrad]
	140649815690880 -> 140649815690832
	140649816692496 [label="block_3_0.bn_3.bias
 (256)" fillcolor=lightblue]
	140649816692496 -> 140649815690880
	140649815690880 [label=AccumulateGrad]
	140649815690784 -> 140649815690688
	140649816693136 [label="block_3_0.actv_3.weight
 (256)" fillcolor=lightblue]
	140649816693136 -> 140649815690784
	140649815690784 [label=AccumulateGrad]
	140649815690640 -> 140649815690544
	140649815690640 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 256, 1, 1)"]
	140649815691072 -> 140649815690640
	140649815691072 [label="UnsqueezeBackward0
------------------
dim: 3"]
	140649815693232 -> 140649815691072
	140649815693232 [label="UnsqueezeBackward0
------------------
dim: 2"]
	140649815691360 -> 140649815693232
	140649815691360 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649815692800 -> 140649815691360
	140649815692800 [label="SumBackward1
---------------------------
dim           :        (0,)
keepdim       :       False
self_sym_sizes: (2, 1, 256)"]
	140649815693136 -> 140649815692800
	140649815693136 [label="StackBackward0
--------------
dim: 0"]
	140649815693328 -> 140649815693136
	140649815693328 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :        (1, 16)
mat1_sym_strides:        (16, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :      (16, 256)
mat2_sym_strides:        (1, 16)"]
	140649815693472 -> 140649815693328
	140649816693456 [label="block_3_0.cbam.channel_attention.shared_mlp.3.bias
 (256)" fillcolor=lightblue]
	140649816693456 -> 140649815693472
	140649815693472 [label=AccumulateGrad]
	140649815693424 -> 140649815693328
	140649815693424 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649815693568 -> 140649815693424
	140649815693568 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 256)
mat1_sym_strides:       (256, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :      (256, 16)
mat2_sym_strides:       (1, 256)"]
	140649815693760 -> 140649815693568
	140649816693296 [label="block_3_0.cbam.channel_attention.shared_mlp.1.bias
 (16)" fillcolor=lightblue]
	140649816693296 -> 140649815693760
	140649815693760 [label=AccumulateGrad]
	140649815693712 -> 140649815693568
	140649815693712 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (1, 256, 1, 1)"]
	140649815693856 -> 140649815693712
	140649815693856 [label="AvgPool2DBackward0
---------------------------------
ceil_mode        :          False
count_include_pad:           True
divisor_override :           None
kernel_size      :         (8, 8)
padding          :         (0, 0)
self             : [saved tensor]
stride           :         (8, 8)"]
	140649815690688 -> 140649815693856
	140649815693664 -> 140649815693568
	140649815693664 [label=TBackward0]
	140649815694000 -> 140649815693664
	140649816693216 [label="block_3_0.cbam.channel_attention.shared_mlp.1.weight
 (16, 256)" fillcolor=lightblue]
	140649816693216 -> 140649815694000
	140649815694000 [label=AccumulateGrad]
	140649815693376 -> 140649815693328
	140649815693376 [label=TBackward0]
	140649815694048 -> 140649815693376
	140649816693376 [label="block_3_0.cbam.channel_attention.shared_mlp.3.weight
 (256, 16)" fillcolor=lightblue]
	140649816693376 -> 140649815694048
	140649815694048 [label=AccumulateGrad]
	140649815693088 -> 140649815693136
	140649815693088 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :        (1, 16)
mat1_sym_strides:        (16, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :      (16, 256)
mat2_sym_strides:        (1, 16)"]
	140649815693472 -> 140649815693088
	140649815693952 -> 140649815693088
	140649815693952 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649815693904 -> 140649815693952
	140649815693904 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 256)
mat1_sym_strides:       (256, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :      (256, 16)
mat2_sym_strides:       (1, 256)"]
	140649815693760 -> 140649815693904
	140649815694192 -> 140649815693904
	140649815694192 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (1, 256, 1, 1)"]
	140649815694336 -> 140649815694192
	140649815694336 [label="MaxPool2DWithIndicesBackward0
-----------------------------
ceil_mode  :          False
dilation   :         (1, 1)
kernel_size:         (8, 8)
padding    :         (0, 0)
result1    : [saved tensor]
self       : [saved tensor]
stride     :         (8, 8)"]
	140649815690688 -> 140649815694336
	140649815694144 -> 140649815693904
	140649815694144 [label=TBackward0]
	140649815694000 -> 140649815694144
	140649815693808 -> 140649815693088
	140649815693808 [label=TBackward0]
	140649815694048 -> 140649815693808
	140649815690496 -> 140649815690400
	140649815690496 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649815691024 -> 140649815690496
	140649815691024 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815692752 -> 140649815691024
	140649815692752 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (3, 3)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815694384 -> 140649815692752
	140649815694384 [label="CatBackward0
------------
dim: 1"]
	140649815694432 -> 140649815694384
	140649815694432 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649815694480 -> 140649815694432
	140649815694480 [label="MaxBackward0
--------------------------
dim       :              1
indices   : [saved tensor]
keepdim   :          False
self_sizes: (1, 256, 8, 8)"]
	140649815690544 -> 140649815694480
	140649815693616 -> 140649815694384
	140649815693616 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649815694528 -> 140649815693616
	140649815694528 [label="MeanBackward1
------------------------------
dim           :           (1,)
keepdim       :          False
self          : [saved tensor]
self_sym_sizes: (1, 256, 8, 8)"]
	140649815690544 -> 140649815694528
	140649815693520 -> 140649815692752
	140649816693696 [label="block_3_0.cbam.spatial_attention.spatial_attention.0.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	140649816693696 -> 140649815693520
	140649815693520 [label=AccumulateGrad]
	140649815691216 -> 140649815691024
	140649816693616 [label="block_3_0.cbam.spatial_attention.spatial_attention.1.weight
 (1)" fillcolor=lightblue]
	140649816693616 -> 140649815691216
	140649815691216 [label=AccumulateGrad]
	140649815690592 -> 140649815691024
	140649816693776 [label="block_3_0.cbam.spatial_attention.spatial_attention.1.bias
 (1)" fillcolor=lightblue]
	140649816693776 -> 140649815690592
	140649815690592 [label=AccumulateGrad]
	140649815690352 -> 140649815688624
	140649815690304 -> 140649815690160
	140649815690304 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815691504 -> 140649815690304
	140649815691504 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815694624 -> 140649815691504
	140649815694624 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815694240 -> 140649815694624
	140649815694240 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            256
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815688624 -> 140649815694240
	140649815694768 -> 140649815694240
	140649816694256 [label="block_3_1.conv_0.depthwise.weight
 (256, 1, 3, 3)" fillcolor=lightblue]
	140649816694256 -> 140649815694768
	140649815694768 [label=AccumulateGrad]
	140649815694288 -> 140649815694624
	140649816694416 [label="block_3_1.conv_0.pointwise.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	140649816694416 -> 140649815694288
	140649815694288 [label=AccumulateGrad]
	140649815690736 -> 140649815691504
	140649816695456 [label="block_3_1.bn_0.weight
 (128)" fillcolor=lightblue]
	140649816695456 -> 140649815690736
	140649815690736 [label=AccumulateGrad]
	140649815693280 -> 140649815691504
	140649816695536 [label="block_3_1.bn_0.bias
 (128)" fillcolor=lightblue]
	140649816695536 -> 140649815693280
	140649815693280 [label=AccumulateGrad]
	140649815690448 -> 140649815690304
	140649816697376 [label="block_3_1.actv_0.weight
 (128)" fillcolor=lightblue]
	140649816697376 -> 140649815690448
	140649815690448 [label=AccumulateGrad]
	140649815690112 -> 140649815690016
	140649815690112 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815694816 -> 140649815690112
	140649815694816 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815694864 -> 140649815694816
	140649815694864 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815694960 -> 140649815694864
	140649815694960 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            384
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815690160 -> 140649815694960
	140649815695104 -> 140649815694960
	140649816694576 [label="block_3_1.conv_1.depthwise.weight
 (384, 1, 3, 3)" fillcolor=lightblue]
	140649816694576 -> 140649815695104
	140649815695104 [label=AccumulateGrad]
	140649815694912 -> 140649815694864
	140649816694736 [label="block_3_1.conv_1.pointwise.weight
 (128, 384, 1, 1)" fillcolor=lightblue]
	140649816694736 -> 140649815694912
	140649815694912 [label=AccumulateGrad]
	140649815694576 -> 140649815694816
	140649816695936 [label="block_3_1.bn_1.weight
 (128)" fillcolor=lightblue]
	140649816695936 -> 140649815694576
	140649815694576 [label=AccumulateGrad]
	140649815694096 -> 140649815694816
	140649816696016 [label="block_3_1.bn_1.bias
 (128)" fillcolor=lightblue]
	140649816696016 -> 140649815694096
	140649815694096 [label=AccumulateGrad]
	140649815690208 -> 140649815690112
	140649816697456 [label="block_3_1.actv_1.weight
 (128)" fillcolor=lightblue]
	140649816697456 -> 140649815690208
	140649815690208 [label=AccumulateGrad]
	140649815689968 -> 140649815689872
	140649815689968 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815695152 -> 140649815689968
	140649815695152 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815695200 -> 140649815695152
	140649815695200 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815695296 -> 140649815695200
	140649815695296 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            512
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815690016 -> 140649815695296
	140649815695440 -> 140649815695296
	140649816694896 [label="block_3_1.conv_2.depthwise.weight
 (512, 1, 3, 3)" fillcolor=lightblue]
	140649816694896 -> 140649815695440
	140649815695440 [label=AccumulateGrad]
	140649815695248 -> 140649815695200
	140649816695056 [label="block_3_1.conv_2.pointwise.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	140649816695056 -> 140649815695248
	140649815695248 [label=AccumulateGrad]
	140649815694720 -> 140649815695152
	140649816696416 [label="block_3_1.bn_2.weight
 (128)" fillcolor=lightblue]
	140649816696416 -> 140649815694720
	140649815694720 [label=AccumulateGrad]
	140649815694672 -> 140649815695152
	140649816696496 [label="block_3_1.bn_2.bias
 (128)" fillcolor=lightblue]
	140649816696496 -> 140649815694672
	140649815694672 [label=AccumulateGrad]
	140649815690256 -> 140649815689968
	140649816697536 [label="block_3_1.actv_2.weight
 (128)" fillcolor=lightblue]
	140649816697536 -> 140649815690256
	140649815690256 [label=AccumulateGrad]
	140649815689824 -> 140649815689728
	140649816695216 [label="block_3_1.conv_3.depthwise.weight
 (640, 1, 3, 3)" fillcolor=lightblue]
	140649816695216 -> 140649815689824
	140649815689824 [label=AccumulateGrad]
	140649815689680 -> 140649815689536
	140649816695376 [label="block_3_1.conv_3.pointwise.weight
 (256, 640, 1, 1)" fillcolor=lightblue]
	140649816695376 -> 140649815689680
	140649815689680 [label=AccumulateGrad]
	140649815689488 -> 140649815689392
	140649816696896 [label="block_3_1.bn_3.weight
 (256)" fillcolor=lightblue]
	140649816696896 -> 140649815689488
	140649815689488 [label=AccumulateGrad]
	140649815689440 -> 140649815689392
	140649816696976 [label="block_3_1.bn_3.bias
 (256)" fillcolor=lightblue]
	140649816696976 -> 140649815689440
	140649815689440 [label=AccumulateGrad]
	140649815689344 -> 140649815689248
	140649816697616 [label="block_3_1.actv_3.weight
 (256)" fillcolor=lightblue]
	140649816697616 -> 140649815689344
	140649815689344 [label=AccumulateGrad]
	140649815689200 -> 140649815689104
	140649815689200 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 256, 1, 1)"]
	140649815689632 -> 140649815689200
	140649815689632 [label="UnsqueezeBackward0
------------------
dim: 3"]
	140649815695488 -> 140649815689632
	140649815695488 [label="UnsqueezeBackward0
------------------
dim: 2"]
	140649815689920 -> 140649815695488
	140649815689920 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649815695056 -> 140649815689920
	140649815695056 [label="SumBackward1
---------------------------
dim           :        (0,)
keepdim       :       False
self_sym_sizes: (2, 1, 256)"]
	140649815695392 -> 140649815695056
	140649815695392 [label="StackBackward0
--------------
dim: 0"]
	140649815695584 -> 140649815695392
	140649815695584 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :        (1, 16)
mat1_sym_strides:        (16, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :      (16, 256)
mat2_sym_strides:        (1, 16)"]
	140649815695728 -> 140649815695584
	140649816697936 [label="block_3_1.cbam.channel_attention.shared_mlp.3.bias
 (256)" fillcolor=lightblue]
	140649816697936 -> 140649815695728
	140649815695728 [label=AccumulateGrad]
	140649815695680 -> 140649815695584
	140649815695680 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649815695824 -> 140649815695680
	140649815695824 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 256)
mat1_sym_strides:       (256, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :      (256, 16)
mat2_sym_strides:       (1, 256)"]
	140649815696016 -> 140649815695824
	140649816697776 [label="block_3_1.cbam.channel_attention.shared_mlp.1.bias
 (16)" fillcolor=lightblue]
	140649816697776 -> 140649815696016
	140649815696016 [label=AccumulateGrad]
	140649815695968 -> 140649815695824
	140649815695968 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (1, 256, 1, 1)"]
	140649815696112 -> 140649815695968
	140649815696112 [label="AvgPool2DBackward0
---------------------------------
ceil_mode        :          False
count_include_pad:           True
divisor_override :           None
kernel_size      :         (8, 8)
padding          :         (0, 0)
self             : [saved tensor]
stride           :         (8, 8)"]
	140649815689248 -> 140649815696112
	140649815695920 -> 140649815695824
	140649815695920 [label=TBackward0]
	140649815696256 -> 140649815695920
	140649816697696 [label="block_3_1.cbam.channel_attention.shared_mlp.1.weight
 (16, 256)" fillcolor=lightblue]
	140649816697696 -> 140649815696256
	140649815696256 [label=AccumulateGrad]
	140649815695632 -> 140649815695584
	140649815695632 [label=TBackward0]
	140649815696304 -> 140649815695632
	140649816697856 [label="block_3_1.cbam.channel_attention.shared_mlp.3.weight
 (256, 16)" fillcolor=lightblue]
	140649816697856 -> 140649815696304
	140649815696304 [label=AccumulateGrad]
	140649815695344 -> 140649815695392
	140649815695344 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :        (1, 16)
mat1_sym_strides:        (16, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :      (16, 256)
mat2_sym_strides:        (1, 16)"]
	140649815695728 -> 140649815695344
	140649815696352 -> 140649815695344
	140649815696352 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649815696208 -> 140649815696352
	140649815696208 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 256)
mat1_sym_strides:       (256, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :      (256, 16)
mat2_sym_strides:       (1, 256)"]
	140649815696016 -> 140649815696208
	140649815696496 -> 140649815696208
	140649815696496 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (1, 256, 1, 1)"]
	140649815696640 -> 140649815696496
	140649815696640 [label="MaxPool2DWithIndicesBackward0
-----------------------------
ceil_mode  :          False
dilation   :         (1, 1)
kernel_size:         (8, 8)
padding    :         (0, 0)
result1    : [saved tensor]
self       : [saved tensor]
stride     :         (8, 8)"]
	140649815689248 -> 140649815696640
	140649815696448 -> 140649815696208
	140649815696448 [label=TBackward0]
	140649815696256 -> 140649815696448
	140649815696064 -> 140649815695344
	140649815696064 [label=TBackward0]
	140649815696304 -> 140649815696064
	140649815689056 -> 140649815688576
	140649815689056 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649815689584 -> 140649815689056
	140649815689584 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815695008 -> 140649815689584
	140649815695008 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (3, 3)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815696688 -> 140649815695008
	140649815696688 [label="CatBackward0
------------
dim: 1"]
	140649815696736 -> 140649815696688
	140649815696736 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649815696784 -> 140649815696736
	140649815696784 [label="MaxBackward0
--------------------------
dim       :              1
indices   : [saved tensor]
keepdim   :          False
self_sizes: (1, 256, 8, 8)"]
	140649815689104 -> 140649815696784
	140649815695872 -> 140649815696688
	140649815695872 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649815696832 -> 140649815695872
	140649815696832 [label="MeanBackward1
------------------------------
dim           :           (1,)
keepdim       :          False
self          : [saved tensor]
self_sym_sizes: (1, 256, 8, 8)"]
	140649815689104 -> 140649815696832
	140649815695776 -> 140649815695008
	140649816698176 [label="block_3_1.cbam.spatial_attention.spatial_attention.0.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	140649816698176 -> 140649815695776
	140649815695776 [label=AccumulateGrad]
	140649815689776 -> 140649815689584
	140649816698096 [label="block_3_1.cbam.spatial_attention.spatial_attention.1.weight
 (1)" fillcolor=lightblue]
	140649816698096 -> 140649815689776
	140649815689776 [label=AccumulateGrad]
	140649815689152 -> 140649815689584
	140649816698256 [label="block_3_1.cbam.spatial_attention.spatial_attention.1.bias
 (1)" fillcolor=lightblue]
	140649816698256 -> 140649815689152
	140649815689152 [label=AccumulateGrad]
	140649815688624 -> 140649815688960
	140649815687568 -> 140649815550864
	140649816698976 [label="up_2.conv_t.weight
 (256, 256, 2, 2)" fillcolor=lightblue]
	140649816698976 -> 140649815687568
	140649815687568 [label=AccumulateGrad]
	140649815687616 -> 140649815550864
	140649816699056 [label="up_2.conv_t.bias
 (256)" fillcolor=lightblue]
	140649816699056 -> 140649815687616
	140649815687616 [label=AccumulateGrad]
	140649815549952 -> 140649815549712
	140649816699216 [label="up_2.actv_t.weight
 (256)" fillcolor=lightblue]
	140649816699216 -> 140649815549952
	140649815549952 [label=AccumulateGrad]
	140649815549568 -> 140649815549472
	140649816698656 [label="up_2.conv.depthwise.weight
 (384, 1, 3, 3)" fillcolor=lightblue]
	140649816698656 -> 140649815549568
	140649815549568 [label=AccumulateGrad]
	140649815549424 -> 140649815549328
	140649816698816 [label="up_2.conv.pointwise.weight
 (128, 384, 1, 1)" fillcolor=lightblue]
	140649816698816 -> 140649815549424
	140649815549424 [label=AccumulateGrad]
	140649815549280 -> 140649815547840
	140649816699136 [label="up_2.actv.weight
 (128)" fillcolor=lightblue]
	140649816699136 -> 140649815549280
	140649815549280 [label=AccumulateGrad]
	140649815549232 -> 140649815549088
	140649815549232 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815549664 -> 140649815549232
	140649815549664 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815549520 -> 140649815549664
	140649815549520 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815695536 -> 140649815549520
	140649815695536 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            128
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815547840 -> 140649815695536
	140649815696880 -> 140649815695536
	140649816699376 [label="block_2_2.conv_0.depthwise.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	140649816699376 -> 140649815696880
	140649815696880 [label=AccumulateGrad]
	140649815690064 -> 140649815549520
	140649816699536 [label="block_2_2.conv_0.pointwise.weight
 (64, 128, 1, 1)" fillcolor=lightblue]
	140649816699536 -> 140649815690064
	140649815690064 [label=AccumulateGrad]
	140649815549376 -> 140649815549664
	140649816700576 [label="block_2_2.bn_0.weight
 (64)" fillcolor=lightblue]
	140649816700576 -> 140649815549376
	140649815549376 [label=AccumulateGrad]
	140649815689008 -> 140649815549664
	140649816700656 [label="block_2_2.bn_0.bias
 (64)" fillcolor=lightblue]
	140649816700656 -> 140649815689008
	140649815689008 [label=AccumulateGrad]
	140649815549808 -> 140649815549232
	140649816702496 [label="block_2_2.actv_0.weight
 (64)" fillcolor=lightblue]
	140649816702496 -> 140649815549808
	140649815549808 [label=AccumulateGrad]
	140649815549040 -> 140649815548944
	140649815549040 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815549136 -> 140649815549040
	140649815549136 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815696592 -> 140649815549136
	140649815696592 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815696976 -> 140649815696592
	140649815696976 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            192
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815549088 -> 140649815696976
	140649815697120 -> 140649815696976
	140649816699696 [label="block_2_2.conv_1.depthwise.weight
 (192, 1, 3, 3)" fillcolor=lightblue]
	140649816699696 -> 140649815697120
	140649815697120 [label=AccumulateGrad]
	140649815696544 -> 140649815696592
	140649816699856 [label="block_2_2.conv_1.pointwise.weight
 (64, 192, 1, 1)" fillcolor=lightblue]
	140649816699856 -> 140649815696544
	140649815696544 [label=AccumulateGrad]
	140649815688432 -> 140649815549136
	140649816701056 [label="block_2_2.bn_1.weight
 (64)" fillcolor=lightblue]
	140649816701056 -> 140649815688432
	140649815688432 [label=AccumulateGrad]
	140649815688912 -> 140649815549136
	140649816701136 [label="block_2_2.bn_1.bias
 (64)" fillcolor=lightblue]
	140649816701136 -> 140649815688912
	140649815688912 [label=AccumulateGrad]
	140649815549184 -> 140649815549040
	140649816702576 [label="block_2_2.actv_1.weight
 (64)" fillcolor=lightblue]
	140649816702576 -> 140649815549184
	140649815549184 [label=AccumulateGrad]
	140649815548896 -> 140649815548800
	140649815548896 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815548992 -> 140649815548896
	140649815548992 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815697216 -> 140649815548992
	140649815697216 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815697312 -> 140649815697216
	140649815697312 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            256
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815548944 -> 140649815697312
	140649815697456 -> 140649815697312
	140649816700016 [label="block_2_2.conv_2.depthwise.weight
 (256, 1, 3, 3)" fillcolor=lightblue]
	140649816700016 -> 140649815697456
	140649815697456 [label=AccumulateGrad]
	140649815697264 -> 140649815697216
	140649816700176 [label="block_2_2.conv_2.pointwise.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	140649816700176 -> 140649815697264
	140649815697264 [label=AccumulateGrad]
	140649815696928 -> 140649815548992
	140649816701536 [label="block_2_2.bn_2.weight
 (64)" fillcolor=lightblue]
	140649816701536 -> 140649815696928
	140649815696928 [label=AccumulateGrad]
	140649815689296 -> 140649815548992
	140649816701616 [label="block_2_2.bn_2.bias
 (64)" fillcolor=lightblue]
	140649816701616 -> 140649815689296
	140649815689296 [label=AccumulateGrad]
	140649815697168 -> 140649815548896
	140649816702656 [label="block_2_2.actv_2.weight
 (64)" fillcolor=lightblue]
	140649816702656 -> 140649815697168
	140649815697168 [label=AccumulateGrad]
	140649815548752 -> 140649815548656
	140649816700336 [label="block_2_2.conv_3.depthwise.weight
 (320, 1, 3, 3)" fillcolor=lightblue]
	140649816700336 -> 140649815548752
	140649815548752 [label=AccumulateGrad]
	140649815548608 -> 140649815548464
	140649816700496 [label="block_2_2.conv_3.pointwise.weight
 (128, 320, 1, 1)" fillcolor=lightblue]
	140649816700496 -> 140649815548608
	140649815548608 [label=AccumulateGrad]
	140649815548416 -> 140649815548320
	140649816702016 [label="block_2_2.bn_3.weight
 (128)" fillcolor=lightblue]
	140649816702016 -> 140649815548416
	140649815548416 [label=AccumulateGrad]
	140649815548368 -> 140649815548320
	140649816702096 [label="block_2_2.bn_3.bias
 (128)" fillcolor=lightblue]
	140649816702096 -> 140649815548368
	140649815548368 [label=AccumulateGrad]
	140649815548272 -> 140649815548176
	140649816702736 [label="block_2_2.actv_3.weight
 (128)" fillcolor=lightblue]
	140649816702736 -> 140649815548272
	140649815548272 [label=AccumulateGrad]
	140649815548128 -> 140649815548032
	140649815548128 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 128, 1, 1)"]
	140649815548560 -> 140649815548128
	140649815548560 [label="UnsqueezeBackward0
------------------
dim: 3"]
	140649815548704 -> 140649815548560
	140649815548704 [label="UnsqueezeBackward0
------------------
dim: 2"]
	140649815548224 -> 140649815548704
	140649815548224 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649815697072 -> 140649815548224
	140649815697072 [label="SumBackward1
---------------------------
dim           :        (0,)
keepdim       :       False
self_sym_sizes: (2, 1, 128)"]
	140649815697408 -> 140649815697072
	140649815697408 [label="StackBackward0
--------------
dim: 0"]
	140649815697600 -> 140649815697408
	140649815697600 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :         (1, 8)
mat1_sym_strides:         (8, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :       (8, 128)
mat2_sym_strides:         (1, 8)"]
	140649815697744 -> 140649815697600
	140649814868112 [label="block_2_2.cbam.channel_attention.shared_mlp.3.bias
 (128)" fillcolor=lightblue]
	140649814868112 -> 140649815697744
	140649815697744 [label=AccumulateGrad]
	140649815697696 -> 140649815697600
	140649815697696 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649815697840 -> 140649815697696
	140649815697840 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 128)
mat1_sym_strides:       (128, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :       (128, 8)
mat2_sym_strides:       (1, 128)"]
	140649815698032 -> 140649815697840
	140649816702896 [label="block_2_2.cbam.channel_attention.shared_mlp.1.bias
 (8)" fillcolor=lightblue]
	140649816702896 -> 140649815698032
	140649815698032 [label=AccumulateGrad]
	140649815697984 -> 140649815697840
	140649815697984 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (1, 128, 1, 1)"]
	140649815698128 -> 140649815697984
	140649815698128 [label="AvgPool2DBackward0
---------------------------------
ceil_mode        :          False
count_include_pad:           True
divisor_override :           None
kernel_size      :       (16, 16)
padding          :         (0, 0)
self             : [saved tensor]
stride           :       (16, 16)"]
	140649815548176 -> 140649815698128
	140649815697936 -> 140649815697840
	140649815697936 [label=TBackward0]
	140649815698272 -> 140649815697936
	140649816702816 [label="block_2_2.cbam.channel_attention.shared_mlp.1.weight
 (8, 128)" fillcolor=lightblue]
	140649816702816 -> 140649815698272
	140649815698272 [label=AccumulateGrad]
	140649815697648 -> 140649815697600
	140649815697648 [label=TBackward0]
	140649815698320 -> 140649815697648
	140649814868032 [label="block_2_2.cbam.channel_attention.shared_mlp.3.weight
 (128, 8)" fillcolor=lightblue]
	140649814868032 -> 140649815698320
	140649815698320 [label=AccumulateGrad]
	140649815697360 -> 140649815697408
	140649815697360 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :         (1, 8)
mat1_sym_strides:         (8, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :       (8, 128)
mat2_sym_strides:         (1, 8)"]
	140649815697744 -> 140649815697360
	140649815698224 -> 140649815697360
	140649815698224 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649815698176 -> 140649815698224
	140649815698176 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 128)
mat1_sym_strides:       (128, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :       (128, 8)
mat2_sym_strides:       (1, 128)"]
	140649815698032 -> 140649815698176
	140649815698464 -> 140649815698176
	140649815698464 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (1, 128, 1, 1)"]
	140649815698608 -> 140649815698464
	140649815698608 [label="MaxPool2DWithIndicesBackward0
-----------------------------
ceil_mode  :          False
dilation   :         (1, 1)
kernel_size:       (16, 16)
padding    :         (0, 0)
result1    : [saved tensor]
self       : [saved tensor]
stride     :       (16, 16)"]
	140649815548176 -> 140649815698608
	140649815698416 -> 140649815698176
	140649815698416 [label=TBackward0]
	140649815698272 -> 140649815698416
	140649815698080 -> 140649815697360
	140649815698080 [label=TBackward0]
	140649815698320 -> 140649815698080
	140649815547984 -> 140649815547888
	140649815547984 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649815548512 -> 140649815547984
	140649815548512 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815548080 -> 140649815548512
	140649815548080 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (3, 3)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815698656 -> 140649815548080
	140649815698656 [label="CatBackward0
------------
dim: 1"]
	140649815698704 -> 140649815698656
	140649815698704 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649815698752 -> 140649815698704
	140649815698752 [label="MaxBackward0
----------------------------
dim       :                1
indices   :   [saved tensor]
keepdim   :            False
self_sizes: (1, 128, 16, 16)"]
	140649815548032 -> 140649815698752
	140649815697888 -> 140649815698656
	140649815697888 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649815698800 -> 140649815697888
	140649815698800 [label="MeanBackward1
--------------------------------
dim           :             (1,)
keepdim       :            False
self          :   [saved tensor]
self_sym_sizes: (1, 128, 16, 16)"]
	140649815548032 -> 140649815698800
	140649815697792 -> 140649815548080
	140649814868352 [label="block_2_2.cbam.spatial_attention.spatial_attention.0.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	140649814868352 -> 140649815697792
	140649815697792 [label=AccumulateGrad]
	140649815697024 -> 140649815548512
	140649814868272 [label="block_2_2.cbam.spatial_attention.spatial_attention.1.weight
 (1)" fillcolor=lightblue]
	140649814868272 -> 140649815697024
	140649815697024 [label=AccumulateGrad]
	140649815696400 -> 140649815548512
	140649814868432 [label="block_2_2.cbam.spatial_attention.spatial_attention.1.bias
 (1)" fillcolor=lightblue]
	140649814868432 -> 140649815696400
	140649815696400 [label=AccumulateGrad]
	140649815547840 -> 140649815546112
	140649815547792 -> 140649815547648
	140649815547792 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815548848 -> 140649815547792
	140649815548848 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815698896 -> 140649815548848
	140649815698896 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815698512 -> 140649815698896
	140649815698512 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            128
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815546112 -> 140649815698512
	140649815699040 -> 140649815698512
	140649814868992 [label="block_2_3.conv_0.depthwise.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	140649814868992 -> 140649815699040
	140649815699040 [label=AccumulateGrad]
	140649815698560 -> 140649815698896
	140649814869152 [label="block_2_3.conv_0.pointwise.weight
 (64, 128, 1, 1)" fillcolor=lightblue]
	140649814869152 -> 140649815698560
	140649815698560 [label=AccumulateGrad]
	140649815697504 -> 140649815548848
	140649814870192 [label="block_2_3.bn_0.weight
 (64)" fillcolor=lightblue]
	140649814870192 -> 140649815697504
	140649815697504 [label=AccumulateGrad]
	140649815697552 -> 140649815548848
	140649814870272 [label="block_2_3.bn_0.bias
 (64)" fillcolor=lightblue]
	140649814870272 -> 140649815697552
	140649815697552 [label=AccumulateGrad]
	140649815547936 -> 140649815547792
	140649814872112 [label="block_2_3.actv_0.weight
 (64)" fillcolor=lightblue]
	140649814872112 -> 140649815547936
	140649815547936 [label=AccumulateGrad]
	140649815547600 -> 140649815547504
	140649815547600 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815547696 -> 140649815547600
	140649815547696 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815699136 -> 140649815547696
	140649815699136 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815699232 -> 140649815699136
	140649815699232 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            192
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815547648 -> 140649815699232
	140649815699376 -> 140649815699232
	140649814869312 [label="block_2_3.conv_1.depthwise.weight
 (192, 1, 3, 3)" fillcolor=lightblue]
	140649814869312 -> 140649815699376
	140649815699376 [label=AccumulateGrad]
	140649815699184 -> 140649815699136
	140649814869472 [label="block_2_3.conv_1.pointwise.weight
 (64, 192, 1, 1)" fillcolor=lightblue]
	140649814869472 -> 140649815699184
	140649815699184 [label=AccumulateGrad]
	140649815698848 -> 140649815547696
	140649814870672 [label="block_2_3.bn_1.weight
 (64)" fillcolor=lightblue]
	140649814870672 -> 140649815698848
	140649815698848 [label=AccumulateGrad]
	140649815698368 -> 140649815547696
	140649814870752 [label="block_2_3.bn_1.bias
 (64)" fillcolor=lightblue]
	140649814870752 -> 140649815698368
	140649815698368 [label=AccumulateGrad]
	140649815547744 -> 140649815547600
	140649814872192 [label="block_2_3.actv_1.weight
 (64)" fillcolor=lightblue]
	140649814872192 -> 140649815547744
	140649815547744 [label=AccumulateGrad]
	140649815547456 -> 140649815547360
	140649815547456 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815547552 -> 140649815547456
	140649815547552 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815699472 -> 140649815547552
	140649815699472 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815699568 -> 140649815699472
	140649815699568 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            256
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815547504 -> 140649815699568
	140649815699712 -> 140649815699568
	140649814869632 [label="block_2_3.conv_2.depthwise.weight
 (256, 1, 3, 3)" fillcolor=lightblue]
	140649814869632 -> 140649815699712
	140649815699712 [label=AccumulateGrad]
	140649815699520 -> 140649815699472
	140649814869792 [label="block_2_3.conv_2.pointwise.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	140649814869792 -> 140649815699520
	140649815699520 [label=AccumulateGrad]
	140649815698992 -> 140649815547552
	140649814871152 [label="block_2_3.bn_2.weight
 (64)" fillcolor=lightblue]
	140649814871152 -> 140649815698992
	140649815698992 [label=AccumulateGrad]
	140649815698944 -> 140649815547552
	140649814871232 [label="block_2_3.bn_2.bias
 (64)" fillcolor=lightblue]
	140649814871232 -> 140649815698944
	140649815698944 [label=AccumulateGrad]
	140649815699424 -> 140649815547456
	140649814872272 [label="block_2_3.actv_2.weight
 (64)" fillcolor=lightblue]
	140649814872272 -> 140649815699424
	140649815699424 [label=AccumulateGrad]
	140649815547312 -> 140649815547216
	140649814869952 [label="block_2_3.conv_3.depthwise.weight
 (320, 1, 3, 3)" fillcolor=lightblue]
	140649814869952 -> 140649815547312
	140649815547312 [label=AccumulateGrad]
	140649815547168 -> 140649815547024
	140649814870112 [label="block_2_3.conv_3.pointwise.weight
 (128, 320, 1, 1)" fillcolor=lightblue]
	140649814870112 -> 140649815547168
	140649815547168 [label=AccumulateGrad]
	140649815546976 -> 140649815546880
	140649814871632 [label="block_2_3.bn_3.weight
 (128)" fillcolor=lightblue]
	140649814871632 -> 140649815546976
	140649815546976 [label=AccumulateGrad]
	140649815546928 -> 140649815546880
	140649814871712 [label="block_2_3.bn_3.bias
 (128)" fillcolor=lightblue]
	140649814871712 -> 140649815546928
	140649815546928 [label=AccumulateGrad]
	140649815546832 -> 140649815546736
	140649814872352 [label="block_2_3.actv_3.weight
 (128)" fillcolor=lightblue]
	140649814872352 -> 140649815546832
	140649815546832 [label=AccumulateGrad]
	140649815546688 -> 140649815546592
	140649815546688 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 128, 1, 1)"]
	140649815547120 -> 140649815546688
	140649815547120 [label="UnsqueezeBackward0
------------------
dim: 3"]
	140649815547264 -> 140649815547120
	140649815547264 [label="UnsqueezeBackward0
------------------
dim: 2"]
	140649815546784 -> 140649815547264
	140649815546784 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649815699328 -> 140649815546784
	140649815699328 [label="SumBackward1
---------------------------
dim           :        (0,)
keepdim       :       False
self_sym_sizes: (2, 1, 128)"]
	140649815699664 -> 140649815699328
	140649815699664 [label="StackBackward0
--------------
dim: 0"]
	140649815699856 -> 140649815699664
	140649815699856 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :         (1, 8)
mat1_sym_strides:         (8, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :       (8, 128)
mat2_sym_strides:         (1, 8)"]
	140649815700000 -> 140649815699856
	140649814872672 [label="block_2_3.cbam.channel_attention.shared_mlp.3.bias
 (128)" fillcolor=lightblue]
	140649814872672 -> 140649815700000
	140649815700000 [label=AccumulateGrad]
	140649815699952 -> 140649815699856
	140649815699952 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649815700096 -> 140649815699952
	140649815700096 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 128)
mat1_sym_strides:       (128, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :       (128, 8)
mat2_sym_strides:       (1, 128)"]
	140649815700288 -> 140649815700096
	140649814872512 [label="block_2_3.cbam.channel_attention.shared_mlp.1.bias
 (8)" fillcolor=lightblue]
	140649814872512 -> 140649815700288
	140649815700288 [label=AccumulateGrad]
	140649815700240 -> 140649815700096
	140649815700240 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (1, 128, 1, 1)"]
	140649815700384 -> 140649815700240
	140649815700384 [label="AvgPool2DBackward0
---------------------------------
ceil_mode        :          False
count_include_pad:           True
divisor_override :           None
kernel_size      :       (16, 16)
padding          :         (0, 0)
self             : [saved tensor]
stride           :       (16, 16)"]
	140649815546736 -> 140649815700384
	140649815700192 -> 140649815700096
	140649815700192 [label=TBackward0]
	140649815700528 -> 140649815700192
	140649814872432 [label="block_2_3.cbam.channel_attention.shared_mlp.1.weight
 (8, 128)" fillcolor=lightblue]
	140649814872432 -> 140649815700528
	140649815700528 [label=AccumulateGrad]
	140649815699904 -> 140649815699856
	140649815699904 [label=TBackward0]
	140649815700576 -> 140649815699904
	140649814872592 [label="block_2_3.cbam.channel_attention.shared_mlp.3.weight
 (128, 8)" fillcolor=lightblue]
	140649814872592 -> 140649815700576
	140649815700576 [label=AccumulateGrad]
	140649815699616 -> 140649815699664
	140649815699616 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :         (1, 8)
mat1_sym_strides:         (8, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :       (8, 128)
mat2_sym_strides:         (1, 8)"]
	140649815700000 -> 140649815699616
	140649815700480 -> 140649815699616
	140649815700480 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649815700432 -> 140649815700480
	140649815700432 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 128)
mat1_sym_strides:       (128, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :       (128, 8)
mat2_sym_strides:       (1, 128)"]
	140649815700288 -> 140649815700432
	140649815700720 -> 140649815700432
	140649815700720 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (1, 128, 1, 1)"]
	140649815700864 -> 140649815700720
	140649815700864 [label="MaxPool2DWithIndicesBackward0
-----------------------------
ceil_mode  :          False
dilation   :         (1, 1)
kernel_size:       (16, 16)
padding    :         (0, 0)
result1    : [saved tensor]
self       : [saved tensor]
stride     :       (16, 16)"]
	140649815546736 -> 140649815700864
	140649815700672 -> 140649815700432
	140649815700672 [label=TBackward0]
	140649815700528 -> 140649815700672
	140649815700336 -> 140649815699616
	140649815700336 [label=TBackward0]
	140649815700576 -> 140649815700336
	140649815546544 -> 140649815546064
	140649815546544 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649815547072 -> 140649815546544
	140649815547072 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815546640 -> 140649815547072
	140649815546640 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (3, 3)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815700912 -> 140649815546640
	140649815700912 [label="CatBackward0
------------
dim: 1"]
	140649815700960 -> 140649815700912
	140649815700960 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649815701008 -> 140649815700960
	140649815701008 [label="MaxBackward0
----------------------------
dim       :                1
indices   :   [saved tensor]
keepdim   :            False
self_sizes: (1, 128, 16, 16)"]
	140649815546592 -> 140649815701008
	140649815700144 -> 140649815700912
	140649815700144 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649815701056 -> 140649815700144
	140649815701056 [label="MeanBackward1
--------------------------------
dim           :             (1,)
keepdim       :            False
self          :   [saved tensor]
self_sym_sizes: (1, 128, 16, 16)"]
	140649815546592 -> 140649815701056
	140649815700048 -> 140649815546640
	140649814872912 [label="block_2_3.cbam.spatial_attention.spatial_attention.0.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	140649814872912 -> 140649815700048
	140649815700048 [label=AccumulateGrad]
	140649815699280 -> 140649815547072
	140649814872832 [label="block_2_3.cbam.spatial_attention.spatial_attention.1.weight
 (1)" fillcolor=lightblue]
	140649814872832 -> 140649815699280
	140649815699280 [label=AccumulateGrad]
	140649815699088 -> 140649815547072
	140649814872992 [label="block_2_3.cbam.spatial_attention.spatial_attention.1.bias
 (1)" fillcolor=lightblue]
	140649814872992 -> 140649815699088
	140649815699088 [label=AccumulateGrad]
	140649815546112 -> 140649815546448
	140649815545008 -> 140649815457456
	140649814873792 [label="up_1.conv_t.weight
 (128, 128, 2, 2)" fillcolor=lightblue]
	140649814873792 -> 140649815545008
	140649815545008 [label=AccumulateGrad]
	140649815545056 -> 140649815457456
	140649814873872 [label="up_1.conv_t.bias
 (128)" fillcolor=lightblue]
	140649814873872 -> 140649815545056
	140649815545056 [label=AccumulateGrad]
	140649815456544 -> 140649815456304
	140649814874032 [label="up_1.actv_t.weight
 (128)" fillcolor=lightblue]
	140649814874032 -> 140649815456544
	140649815456544 [label=AccumulateGrad]
	140649815456160 -> 140649815456064
	140649814873472 [label="up_1.conv.depthwise.weight
 (192, 1, 3, 3)" fillcolor=lightblue]
	140649814873472 -> 140649815456160
	140649815456160 [label=AccumulateGrad]
	140649815456016 -> 140649815455920
	140649814873632 [label="up_1.conv.pointwise.weight
 (64, 192, 1, 1)" fillcolor=lightblue]
	140649814873632 -> 140649815456016
	140649815456016 [label=AccumulateGrad]
	140649815455872 -> 140649815454432
	140649814873952 [label="up_1.actv.weight
 (64)" fillcolor=lightblue]
	140649814873952 -> 140649815455872
	140649815455872 [label=AccumulateGrad]
	140649815455824 -> 140649815455680
	140649815455824 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815456256 -> 140649815455824
	140649815456256 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815456112 -> 140649815456256
	140649815456112 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815547408 -> 140649815456112
	140649815547408 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             64
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815454432 -> 140649815547408
	140649815701104 -> 140649815547408
	140649814874192 [label="block_1_2.conv_0.depthwise.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	140649814874192 -> 140649815701104
	140649815701104 [label=AccumulateGrad]
	140649815546400 -> 140649815456112
	140649814874352 [label="block_1_2.conv_0.pointwise.weight
 (32, 64, 1, 1)" fillcolor=lightblue]
	140649814874352 -> 140649815546400
	140649815546400 [label=AccumulateGrad]
	140649815455968 -> 140649815456256
	140649814875392 [label="block_1_2.bn_0.weight
 (32)" fillcolor=lightblue]
	140649814875392 -> 140649815455968
	140649815455968 [label=AccumulateGrad]
	140649815546496 -> 140649815456256
	140649814875472 [label="block_1_2.bn_0.bias
 (32)" fillcolor=lightblue]
	140649814875472 -> 140649815546496
	140649815546496 [label=AccumulateGrad]
	140649815456400 -> 140649815455824
	140649814877312 [label="block_1_2.actv_0.weight
 (32)" fillcolor=lightblue]
	140649814877312 -> 140649815456400
	140649815456400 [label=AccumulateGrad]
	140649815455632 -> 140649815455536
	140649815455632 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815545920 -> 140649815455632
	140649815545920 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815700816 -> 140649815545920
	140649815700816 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815701200 -> 140649815700816
	140649815701200 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             96
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815455680 -> 140649815701200
	140649815701344 -> 140649815701200
	140649814874512 [label="block_1_2.conv_1.depthwise.weight
 (96, 1, 3, 3)" fillcolor=lightblue]
	140649814874512 -> 140649815701344
	140649815701344 [label=AccumulateGrad]
	140649815700768 -> 140649815700816
	140649814874672 [label="block_1_2.conv_1.pointwise.weight
 (32, 96, 1, 1)" fillcolor=lightblue]
	140649814874672 -> 140649815700768
	140649815700768 [label=AccumulateGrad]
	140649815699808 -> 140649815545920
	140649814875872 [label="block_1_2.bn_1.weight
 (32)" fillcolor=lightblue]
	140649814875872 -> 140649815699808
	140649815699808 [label=AccumulateGrad]
	140649815700624 -> 140649815545920
	140649814875952 [label="block_1_2.bn_1.bias
 (32)" fillcolor=lightblue]
	140649814875952 -> 140649815700624
	140649815700624 [label=AccumulateGrad]
	140649815455728 -> 140649815455632
	140649814877392 [label="block_1_2.actv_1.weight
 (32)" fillcolor=lightblue]
	140649814877392 -> 140649815455728
	140649815455728 [label=AccumulateGrad]
	140649815455488 -> 140649815455392
	140649815455488 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815455776 -> 140649815455488
	140649815455776 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815701440 -> 140649815455776
	140649815701440 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815701536 -> 140649815701440
	140649815701536 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            128
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815455536 -> 140649815701536
	140649815701680 -> 140649815701536
	140649814874832 [label="block_1_2.conv_2.depthwise.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	140649814874832 -> 140649815701680
	140649815701680 [label=AccumulateGrad]
	140649815701488 -> 140649815701440
	140649814874992 [label="block_1_2.conv_2.pointwise.weight
 (32, 128, 1, 1)" fillcolor=lightblue]
	140649814874992 -> 140649815701488
	140649815701488 [label=AccumulateGrad]
	140649815701152 -> 140649815455776
	140649814876352 [label="block_1_2.bn_2.weight
 (32)" fillcolor=lightblue]
	140649814876352 -> 140649815701152
	140649815701152 [label=AccumulateGrad]
	140649815699760 -> 140649815455776
	140649814876432 [label="block_1_2.bn_2.bias
 (32)" fillcolor=lightblue]
	140649814876432 -> 140649815699760
	140649815699760 [label=AccumulateGrad]
	140649815455584 -> 140649815455488
	140649814877472 [label="block_1_2.actv_2.weight
 (32)" fillcolor=lightblue]
	140649814877472 -> 140649815455584
	140649815455584 [label=AccumulateGrad]
	140649815455344 -> 140649815455248
	140649814875152 [label="block_1_2.conv_3.depthwise.weight
 (160, 1, 3, 3)" fillcolor=lightblue]
	140649814875152 -> 140649815455344
	140649815455344 [label=AccumulateGrad]
	140649815455200 -> 140649815455056
	140649814875312 [label="block_1_2.conv_3.pointwise.weight
 (64, 160, 1, 1)" fillcolor=lightblue]
	140649814875312 -> 140649815455200
	140649815455200 [label=AccumulateGrad]
	140649815455008 -> 140649815454912
	140649814876832 [label="block_1_2.bn_3.weight
 (64)" fillcolor=lightblue]
	140649814876832 -> 140649815455008
	140649815455008 [label=AccumulateGrad]
	140649815454960 -> 140649815454912
	140649814876912 [label="block_1_2.bn_3.bias
 (64)" fillcolor=lightblue]
	140649814876912 -> 140649815454960
	140649815454960 [label=AccumulateGrad]
	140649815454864 -> 140649815454768
	140649814877552 [label="block_1_2.actv_3.weight
 (64)" fillcolor=lightblue]
	140649814877552 -> 140649815454864
	140649815454864 [label=AccumulateGrad]
	140649815454720 -> 140649815454624
	140649815454720 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (1, 64, 1, 1)"]
	140649815455152 -> 140649815454720
	140649815455152 [label="UnsqueezeBackward0
------------------
dim: 3"]
	140649815455296 -> 140649815455152
	140649815455296 [label="UnsqueezeBackward0
------------------
dim: 2"]
	140649815454816 -> 140649815455296
	140649815454816 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649815701296 -> 140649815454816
	140649815701296 [label="SumBackward1
--------------------------
dim           :       (0,)
keepdim       :      False
self_sym_sizes: (2, 1, 64)"]
	140649815701632 -> 140649815701296
	140649815701632 [label="StackBackward0
--------------
dim: 0"]
	140649815701824 -> 140649815701632
	140649815701824 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :         (1, 4)
mat1_sym_strides:         (4, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (4, 64)
mat2_sym_strides:         (1, 4)"]
	140649815701968 -> 140649815701824
	140649814877872 [label="block_1_2.cbam.channel_attention.shared_mlp.3.bias
 (64)" fillcolor=lightblue]
	140649814877872 -> 140649815701968
	140649815701968 [label=AccumulateGrad]
	140649815701920 -> 140649815701824
	140649815701920 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649815702064 -> 140649815701920
	140649815702064 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :        (1, 64)
mat1_sym_strides:        (64, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (64, 4)
mat2_sym_strides:        (1, 64)"]
	140649815702256 -> 140649815702064
	140649814877712 [label="block_1_2.cbam.channel_attention.shared_mlp.1.bias
 (4)" fillcolor=lightblue]
	140649814877712 -> 140649815702256
	140649815702256 [label=AccumulateGrad]
	140649815702208 -> 140649815702064
	140649815702208 [label="ReshapeAliasBackward0
-----------------------------
self_sym_sizes: (1, 64, 1, 1)"]
	140649815702352 -> 140649815702208
	140649815702352 [label="AvgPool2DBackward0
---------------------------------
ceil_mode        :          False
count_include_pad:           True
divisor_override :           None
kernel_size      :       (32, 32)
padding          :         (0, 0)
self             : [saved tensor]
stride           :       (32, 32)"]
	140649815454768 -> 140649815702352
	140649815702160 -> 140649815702064
	140649815702160 [label=TBackward0]
	140649815702496 -> 140649815702160
	140649814877632 [label="block_1_2.cbam.channel_attention.shared_mlp.1.weight
 (4, 64)" fillcolor=lightblue]
	140649814877632 -> 140649815702496
	140649815702496 [label=AccumulateGrad]
	140649815701872 -> 140649815701824
	140649815701872 [label=TBackward0]
	140649815702544 -> 140649815701872
	140649814877792 [label="block_1_2.cbam.channel_attention.shared_mlp.3.weight
 (64, 4)" fillcolor=lightblue]
	140649814877792 -> 140649815702544
	140649815702544 [label=AccumulateGrad]
	140649815701584 -> 140649815701632
	140649815701584 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :         (1, 4)
mat1_sym_strides:         (4, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (4, 64)
mat2_sym_strides:         (1, 4)"]
	140649815701968 -> 140649815701584
	140649815702448 -> 140649815701584
	140649815702448 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649815702400 -> 140649815702448
	140649815702400 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :        (1, 64)
mat1_sym_strides:        (64, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (64, 4)
mat2_sym_strides:        (1, 64)"]
	140649815702256 -> 140649815702400
	140649815702688 -> 140649815702400
	140649815702688 [label="ReshapeAliasBackward0
-----------------------------
self_sym_sizes: (1, 64, 1, 1)"]
	140649815702832 -> 140649815702688
	140649815702832 [label="MaxPool2DWithIndicesBackward0
-----------------------------
ceil_mode  :          False
dilation   :         (1, 1)
kernel_size:       (32, 32)
padding    :         (0, 0)
result1    : [saved tensor]
self       : [saved tensor]
stride     :       (32, 32)"]
	140649815454768 -> 140649815702832
	140649815702640 -> 140649815702400
	140649815702640 [label=TBackward0]
	140649815702496 -> 140649815702640
	140649815702304 -> 140649815701584
	140649815702304 [label=TBackward0]
	140649815702544 -> 140649815702304
	140649815454576 -> 140649815454480
	140649815454576 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649815455104 -> 140649815454576
	140649815455104 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815454672 -> 140649815455104
	140649815454672 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (3, 3)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815702880 -> 140649815454672
	140649815702880 [label="CatBackward0
------------
dim: 1"]
	140649815702928 -> 140649815702880
	140649815702928 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649815702976 -> 140649815702928
	140649815702976 [label="MaxBackward0
---------------------------
dim       :               1
indices   :  [saved tensor]
keepdim   :           False
self_sizes: (1, 64, 32, 32)"]
	140649815454624 -> 140649815702976
	140649815702112 -> 140649815702880
	140649815702112 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649815703024 -> 140649815702112
	140649815703024 [label="MeanBackward1
-------------------------------
dim           :            (1,)
keepdim       :           False
self          :  [saved tensor]
self_sym_sizes: (1, 64, 32, 32)"]
	140649815454624 -> 140649815703024
	140649815702016 -> 140649815454672
	140649814878112 [label="block_1_2.cbam.spatial_attention.spatial_attention.0.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	140649814878112 -> 140649815702016
	140649815702016 [label=AccumulateGrad]
	140649815701248 -> 140649815455104
	140649814878032 [label="block_1_2.cbam.spatial_attention.spatial_attention.1.weight
 (1)" fillcolor=lightblue]
	140649814878032 -> 140649815701248
	140649815701248 [label=AccumulateGrad]
	140649815701392 -> 140649815455104
	140649814878192 [label="block_1_2.cbam.spatial_attention.spatial_attention.1.bias
 (1)" fillcolor=lightblue]
	140649814878192 -> 140649815701392
	140649815701392 [label=AccumulateGrad]
	140649815454432 -> 140649815452704
	140649815454384 -> 140649815454240
	140649815454384 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815455440 -> 140649815454384
	140649815455440 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815703120 -> 140649815455440
	140649815703120 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815702736 -> 140649815703120
	140649815702736 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             64
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815452704 -> 140649815702736
	140649815703264 -> 140649815702736
	140649814878592 [label="block_1_3.conv_0.depthwise.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	140649814878592 -> 140649815703264
	140649815703264 [label=AccumulateGrad]
	140649815702784 -> 140649815703120
	140649814878752 [label="block_1_3.conv_0.pointwise.weight
 (32, 64, 1, 1)" fillcolor=lightblue]
	140649814878752 -> 140649815702784
	140649815702784 [label=AccumulateGrad]
	140649815701728 -> 140649815455440
	140649814879792 [label="block_1_3.bn_0.weight
 (32)" fillcolor=lightblue]
	140649814879792 -> 140649815701728
	140649815701728 [label=AccumulateGrad]
	140649815701776 -> 140649815455440
	140649814879872 [label="block_1_3.bn_0.bias
 (32)" fillcolor=lightblue]
	140649814879872 -> 140649815701776
	140649815701776 [label=AccumulateGrad]
	140649815454528 -> 140649815454384
	140649814881632 [label="block_1_3.actv_0.weight
 (32)" fillcolor=lightblue]
	140649814881632 -> 140649815454528
	140649815454528 [label=AccumulateGrad]
	140649815454192 -> 140649815454096
	140649815454192 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815454288 -> 140649815454192
	140649815454288 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815703360 -> 140649815454288
	140649815703360 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815703456 -> 140649815703360
	140649815703456 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             96
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815454240 -> 140649815703456
	140649815703504 -> 140649815703456
	140649814878912 [label="block_1_3.conv_1.depthwise.weight
 (96, 1, 3, 3)" fillcolor=lightblue]
	140649814878912 -> 140649815703504
	140649815703504 [label=AccumulateGrad]
	140649815703408 -> 140649815703360
	140649814879072 [label="block_1_3.conv_1.pointwise.weight
 (32, 96, 1, 1)" fillcolor=lightblue]
	140649814879072 -> 140649815703408
	140649815703408 [label=AccumulateGrad]
	140649815703072 -> 140649815454288
	140649814880272 [label="block_1_3.bn_1.weight
 (32)" fillcolor=lightblue]
	140649814880272 -> 140649815703072
	140649815703072 [label=AccumulateGrad]
	140649815702592 -> 140649815454288
	140649814880352 [label="block_1_3.bn_1.bias
 (32)" fillcolor=lightblue]
	140649814880352 -> 140649815702592
	140649815702592 [label=AccumulateGrad]
	140649815454336 -> 140649815454192
	140649814881712 [label="block_1_3.actv_1.weight
 (32)" fillcolor=lightblue]
	140649814881712 -> 140649815454336
	140649815454336 [label=AccumulateGrad]
	140649815454048 -> 140649815453952
	140649815454048 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815454144 -> 140649815454048
	140649815454144 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815703216 -> 140649815454144
	140649815703216 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649728852272 -> 140649815703216
	140649728852272 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :            128
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815454096 -> 140649728852272
	140649728852416 -> 140649728852272
	140649814879232 [label="block_1_3.conv_2.depthwise.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	140649814879232 -> 140649728852416
	140649728852416 [label=AccumulateGrad]
	140649728852224 -> 140649815703216
	140649814879392 [label="block_1_3.conv_2.pointwise.weight
 (32, 128, 1, 1)" fillcolor=lightblue]
	140649814879392 -> 140649728852224
	140649728852224 [label=AccumulateGrad]
	140649728852176 -> 140649815454144
	140649814880672 [label="block_1_3.bn_2.weight
 (32)" fillcolor=lightblue]
	140649814880672 -> 140649728852176
	140649728852176 [label=AccumulateGrad]
	140649728852128 -> 140649815454144
	140649814880752 [label="block_1_3.bn_2.bias
 (32)" fillcolor=lightblue]
	140649814880752 -> 140649728852128
	140649728852128 [label=AccumulateGrad]
	140649815703168 -> 140649815454048
	140649814881792 [label="block_1_3.actv_2.weight
 (32)" fillcolor=lightblue]
	140649814881792 -> 140649815703168
	140649815703168 [label=AccumulateGrad]
	140649815453904 -> 140649815453808
	140649814879552 [label="block_1_3.conv_3.depthwise.weight
 (160, 1, 3, 3)" fillcolor=lightblue]
	140649814879552 -> 140649815453904
	140649815453904 [label=AccumulateGrad]
	140649815453760 -> 140649815453616
	140649814879712 [label="block_1_3.conv_3.pointwise.weight
 (64, 160, 1, 1)" fillcolor=lightblue]
	140649814879712 -> 140649815453760
	140649815453760 [label=AccumulateGrad]
	140649815453568 -> 140649815453472
	140649814881152 [label="block_1_3.bn_3.weight
 (64)" fillcolor=lightblue]
	140649814881152 -> 140649815453568
	140649815453568 [label=AccumulateGrad]
	140649815453520 -> 140649815453472
	140649814881232 [label="block_1_3.bn_3.bias
 (64)" fillcolor=lightblue]
	140649814881232 -> 140649815453520
	140649815453520 [label=AccumulateGrad]
	140649815453424 -> 140649815453328
	140649814881872 [label="block_1_3.actv_3.weight
 (64)" fillcolor=lightblue]
	140649814881872 -> 140649815453424
	140649815453424 [label=AccumulateGrad]
	140649815453280 -> 140649815453184
	140649815453280 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (1, 64, 1, 1)"]
	140649815453712 -> 140649815453280
	140649815453712 [label="UnsqueezeBackward0
------------------
dim: 3"]
	140649815453856 -> 140649815453712
	140649815453856 [label="UnsqueezeBackward0
------------------
dim: 2"]
	140649815703312 -> 140649815453856
	140649815703312 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649728852080 -> 140649815703312
	140649728852080 [label="SumBackward1
--------------------------
dim           :       (0,)
keepdim       :      False
self_sym_sizes: (2, 1, 64)"]
	140649728852368 -> 140649728852080
	140649728852368 [label="StackBackward0
--------------
dim: 0"]
	140649728852560 -> 140649728852368
	140649728852560 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :         (1, 4)
mat1_sym_strides:         (4, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (4, 64)
mat2_sym_strides:         (1, 4)"]
	140649728852704 -> 140649728852560
	140649814882192 [label="block_1_3.cbam.channel_attention.shared_mlp.3.bias
 (64)" fillcolor=lightblue]
	140649814882192 -> 140649728852704
	140649728852704 [label=AccumulateGrad]
	140649728852656 -> 140649728852560
	140649728852656 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649728852800 -> 140649728852656
	140649728852800 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :        (1, 64)
mat1_sym_strides:        (64, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (64, 4)
mat2_sym_strides:        (1, 64)"]
	140649728852992 -> 140649728852800
	140649814882032 [label="block_1_3.cbam.channel_attention.shared_mlp.1.bias
 (4)" fillcolor=lightblue]
	140649814882032 -> 140649728852992
	140649728852992 [label=AccumulateGrad]
	140649728852944 -> 140649728852800
	140649728852944 [label="ReshapeAliasBackward0
-----------------------------
self_sym_sizes: (1, 64, 1, 1)"]
	140649728853088 -> 140649728852944
	140649728853088 [label="AvgPool2DBackward0
---------------------------------
ceil_mode        :          False
count_include_pad:           True
divisor_override :           None
kernel_size      :       (32, 32)
padding          :         (0, 0)
self             : [saved tensor]
stride           :       (32, 32)"]
	140649815453328 -> 140649728853088
	140649728852896 -> 140649728852800
	140649728852896 [label=TBackward0]
	140649728853232 -> 140649728852896
	140649814881952 [label="block_1_3.cbam.channel_attention.shared_mlp.1.weight
 (4, 64)" fillcolor=lightblue]
	140649814881952 -> 140649728853232
	140649728853232 [label=AccumulateGrad]
	140649728852608 -> 140649728852560
	140649728852608 [label=TBackward0]
	140649728853280 -> 140649728852608
	140649814882112 [label="block_1_3.cbam.channel_attention.shared_mlp.3.weight
 (64, 4)" fillcolor=lightblue]
	140649814882112 -> 140649728853280
	140649728853280 [label=AccumulateGrad]
	140649728852320 -> 140649728852368
	140649728852320 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :         (1, 4)
mat1_sym_strides:         (4, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (4, 64)
mat2_sym_strides:         (1, 4)"]
	140649728852704 -> 140649728852320
	140649728853184 -> 140649728852320
	140649728853184 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649728853136 -> 140649728853184
	140649728853136 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :        (1, 64)
mat1_sym_strides:        (64, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (64, 4)
mat2_sym_strides:        (1, 64)"]
	140649728852992 -> 140649728853136
	140649728853424 -> 140649728853136
	140649728853424 [label="ReshapeAliasBackward0
-----------------------------
self_sym_sizes: (1, 64, 1, 1)"]
	140649728853568 -> 140649728853424
	140649728853568 [label="MaxPool2DWithIndicesBackward0
-----------------------------
ceil_mode  :          False
dilation   :         (1, 1)
kernel_size:       (32, 32)
padding    :         (0, 0)
result1    : [saved tensor]
self       : [saved tensor]
stride     :       (32, 32)"]
	140649815453328 -> 140649728853568
	140649728853376 -> 140649728853136
	140649728853376 [label=TBackward0]
	140649728853232 -> 140649728853376
	140649728853040 -> 140649728852320
	140649728853040 [label=TBackward0]
	140649728853280 -> 140649728853040
	140649815453136 -> 140649815452656
	140649815453136 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649815453664 -> 140649815453136
	140649815453664 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815453376 -> 140649815453664
	140649815453376 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (3, 3)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649728853616 -> 140649815453376
	140649728853616 [label="CatBackward0
------------
dim: 1"]
	140649728853664 -> 140649728853616
	140649728853664 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649728853712 -> 140649728853664
	140649728853712 [label="MaxBackward0
---------------------------
dim       :               1
indices   :  [saved tensor]
keepdim   :           False
self_sizes: (1, 64, 32, 32)"]
	140649815453184 -> 140649728853712
	140649728852848 -> 140649728853616
	140649728852848 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649728853760 -> 140649728852848
	140649728853760 [label="MeanBackward1
-------------------------------
dim           :            (1,)
keepdim       :           False
self          :  [saved tensor]
self_sym_sizes: (1, 64, 32, 32)"]
	140649815453184 -> 140649728853760
	140649728852752 -> 140649815453376
	140649814882432 [label="block_1_3.cbam.spatial_attention.spatial_attention.0.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	140649814882432 -> 140649728852752
	140649728852752 [label=AccumulateGrad]
	140649815453232 -> 140649815453664
	140649814882352 [label="block_1_3.cbam.spatial_attention.spatial_attention.1.weight
 (1)" fillcolor=lightblue]
	140649814882352 -> 140649815453232
	140649815453232 [label=AccumulateGrad]
	140649728852032 -> 140649815453664
	140649814882512 [label="block_1_3.cbam.spatial_attention.spatial_attention.1.bias
 (1)" fillcolor=lightblue]
	140649814882512 -> 140649728852032
	140649728852032 [label=AccumulateGrad]
	140649815452704 -> 140649815453040
	140649815445072 -> 140649815445840
	140649814883312 [label="up_0.conv_t.weight
 (64, 64, 2, 2)" fillcolor=lightblue]
	140649814883312 -> 140649815445072
	140649815445072 [label=AccumulateGrad]
	140649815451648 -> 140649815445840
	140649814883392 [label="up_0.conv_t.bias
 (64)" fillcolor=lightblue]
	140649814883392 -> 140649815451648
	140649815451648 [label=AccumulateGrad]
	140649815444784 -> 140649815444544
	140649814883552 [label="up_0.actv_t.weight
 (64)" fillcolor=lightblue]
	140649814883552 -> 140649815444784
	140649815444784 [label=AccumulateGrad]
	140649815444400 -> 140649815444304
	140649814882992 [label="up_0.conv.depthwise.weight
 (96, 1, 3, 3)" fillcolor=lightblue]
	140649814882992 -> 140649815444400
	140649815444400 [label=AccumulateGrad]
	140649815444256 -> 140649815444160
	140649814883152 [label="up_0.conv.pointwise.weight
 (32, 96, 1, 1)" fillcolor=lightblue]
	140649814883152 -> 140649815444256
	140649815444256 [label=AccumulateGrad]
	140649815444112 -> 140649815442672
	140649814883472 [label="up_0.actv.weight
 (32)" fillcolor=lightblue]
	140649814883472 -> 140649815444112
	140649815444112 [label=AccumulateGrad]
	140649815444064 -> 140649815443920
	140649815444064 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815444640 -> 140649815444064
	140649815444640 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815444496 -> 140649815444640
	140649815444496 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815454000 -> 140649815444496
	140649815454000 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             32
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815442672 -> 140649815454000
	140649728853808 -> 140649815454000
	140649814883712 [label="block_0_2.conv_0.depthwise.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	140649814883712 -> 140649728853808
	140649728853808 [label=AccumulateGrad]
	140649815452992 -> 140649815444496
	140649814883872 [label="block_0_2.conv_0.pointwise.weight
 (16, 32, 1, 1)" fillcolor=lightblue]
	140649814883872 -> 140649815452992
	140649815452992 [label=AccumulateGrad]
	140649815444352 -> 140649815444640
	140649815163504 [label="block_0_2.bn_0.weight
 (16)" fillcolor=lightblue]
	140649815163504 -> 140649815444352
	140649815444352 [label=AccumulateGrad]
	140649815444208 -> 140649815444640
	140649815163584 [label="block_0_2.bn_0.bias
 (16)" fillcolor=lightblue]
	140649815163584 -> 140649815444208
	140649815444208 [label=AccumulateGrad]
	140649815453088 -> 140649815444064
	140649815165424 [label="block_0_2.actv_0.weight
 (16)" fillcolor=lightblue]
	140649815165424 -> 140649815453088
	140649815453088 [label=AccumulateGrad]
	140649815443872 -> 140649815443776
	140649815443872 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815452512 -> 140649815443872
	140649815452512 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649728853520 -> 140649815452512
	140649728853520 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649728853904 -> 140649728853520
	140649728853904 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             48
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815443920 -> 140649728853904
	140649728854048 -> 140649728853904
	140649814884032 [label="block_0_2.conv_1.depthwise.weight
 (48, 1, 3, 3)" fillcolor=lightblue]
	140649814884032 -> 140649728854048
	140649728854048 [label=AccumulateGrad]
	140649728853472 -> 140649728853520
	140649814884192 [label="block_0_2.conv_1.pointwise.weight
 (16, 48, 1, 1)" fillcolor=lightblue]
	140649814884192 -> 140649728853472
	140649728853472 [label=AccumulateGrad]
	140649728852512 -> 140649815452512
	140649815163984 [label="block_0_2.bn_1.weight
 (16)" fillcolor=lightblue]
	140649815163984 -> 140649728852512
	140649728852512 [label=AccumulateGrad]
	140649728853328 -> 140649815452512
	140649815164064 [label="block_0_2.bn_1.bias
 (16)" fillcolor=lightblue]
	140649815164064 -> 140649728853328
	140649728853328 [label=AccumulateGrad]
	140649815443968 -> 140649815443872
	140649815165504 [label="block_0_2.actv_1.weight
 (16)" fillcolor=lightblue]
	140649815165504 -> 140649815443968
	140649815443968 [label=AccumulateGrad]
	140649815443728 -> 140649815443632
	140649815443728 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815444016 -> 140649815443728
	140649815444016 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649728854144 -> 140649815444016
	140649728854144 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649728854240 -> 140649728854144
	140649728854240 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             64
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815443776 -> 140649728854240
	140649728854384 -> 140649728854240
	140649815162944 [label="block_0_2.conv_2.depthwise.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	140649815162944 -> 140649728854384
	140649728854384 [label=AccumulateGrad]
	140649728854192 -> 140649728854144
	140649815163104 [label="block_0_2.conv_2.pointwise.weight
 (16, 64, 1, 1)" fillcolor=lightblue]
	140649815163104 -> 140649728854192
	140649728854192 [label=AccumulateGrad]
	140649728853856 -> 140649815444016
	140649815164464 [label="block_0_2.bn_2.weight
 (16)" fillcolor=lightblue]
	140649815164464 -> 140649728853856
	140649728853856 [label=AccumulateGrad]
	140649728852464 -> 140649815444016
	140649815164544 [label="block_0_2.bn_2.bias
 (16)" fillcolor=lightblue]
	140649815164544 -> 140649728852464
	140649728852464 [label=AccumulateGrad]
	140649815443824 -> 140649815443728
	140649815165584 [label="block_0_2.actv_2.weight
 (16)" fillcolor=lightblue]
	140649815165584 -> 140649815443824
	140649815443824 [label=AccumulateGrad]
	140649815443584 -> 140649815443488
	140649815163264 [label="block_0_2.conv_3.depthwise.weight
 (80, 1, 3, 3)" fillcolor=lightblue]
	140649815163264 -> 140649815443584
	140649815443584 [label=AccumulateGrad]
	140649815443440 -> 140649815443296
	140649815163424 [label="block_0_2.conv_3.pointwise.weight
 (32, 80, 1, 1)" fillcolor=lightblue]
	140649815163424 -> 140649815443440
	140649815443440 [label=AccumulateGrad]
	140649815443248 -> 140649815443152
	140649815164944 [label="block_0_2.bn_3.weight
 (32)" fillcolor=lightblue]
	140649815164944 -> 140649815443248
	140649815443248 [label=AccumulateGrad]
	140649815443200 -> 140649815443152
	140649815165024 [label="block_0_2.bn_3.bias
 (32)" fillcolor=lightblue]
	140649815165024 -> 140649815443200
	140649815443200 [label=AccumulateGrad]
	140649815443104 -> 140649815443008
	140649815165664 [label="block_0_2.actv_3.weight
 (32)" fillcolor=lightblue]
	140649815165664 -> 140649815443104
	140649815443104 [label=AccumulateGrad]
	140649815442960 -> 140649815442864
	140649815442960 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (1, 32, 1, 1)"]
	140649815443392 -> 140649815442960
	140649815443392 [label="UnsqueezeBackward0
------------------
dim: 3"]
	140649815443536 -> 140649815443392
	140649815443536 [label="UnsqueezeBackward0
------------------
dim: 2"]
	140649815443056 -> 140649815443536
	140649815443056 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649728854000 -> 140649815443056
	140649728854000 [label="SumBackward1
--------------------------
dim           :       (0,)
keepdim       :      False
self_sym_sizes: (2, 1, 32)"]
	140649728854336 -> 140649728854000
	140649728854336 [label="StackBackward0
--------------
dim: 0"]
	140649728854528 -> 140649728854336
	140649728854528 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :         (1, 2)
mat1_sym_strides:         (2, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (2, 32)
mat2_sym_strides:         (1, 2)"]
	140649728854672 -> 140649728854528
	140649815165984 [label="block_0_2.cbam.channel_attention.shared_mlp.3.bias
 (32)" fillcolor=lightblue]
	140649815165984 -> 140649728854672
	140649728854672 [label=AccumulateGrad]
	140649728854624 -> 140649728854528
	140649728854624 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649728854768 -> 140649728854624
	140649728854768 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :        (1, 32)
mat1_sym_strides:        (32, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (32, 2)
mat2_sym_strides:        (1, 32)"]
	140649728854960 -> 140649728854768
	140649815165824 [label="block_0_2.cbam.channel_attention.shared_mlp.1.bias
 (2)" fillcolor=lightblue]
	140649815165824 -> 140649728854960
	140649728854960 [label=AccumulateGrad]
	140649728854912 -> 140649728854768
	140649728854912 [label="ReshapeAliasBackward0
-----------------------------
self_sym_sizes: (1, 32, 1, 1)"]
	140649728855056 -> 140649728854912
	140649728855056 [label="AvgPool2DBackward0
---------------------------------
ceil_mode        :          False
count_include_pad:           True
divisor_override :           None
kernel_size      :       (64, 64)
padding          :         (0, 0)
self             : [saved tensor]
stride           :       (64, 64)"]
	140649815443008 -> 140649728855056
	140649728854864 -> 140649728854768
	140649728854864 [label=TBackward0]
	140649728855200 -> 140649728854864
	140649815165744 [label="block_0_2.cbam.channel_attention.shared_mlp.1.weight
 (2, 32)" fillcolor=lightblue]
	140649815165744 -> 140649728855200
	140649728855200 [label=AccumulateGrad]
	140649728854576 -> 140649728854528
	140649728854576 [label=TBackward0]
	140649728855248 -> 140649728854576
	140649815165904 [label="block_0_2.cbam.channel_attention.shared_mlp.3.weight
 (32, 2)" fillcolor=lightblue]
	140649815165904 -> 140649728855248
	140649728855248 [label=AccumulateGrad]
	140649728854288 -> 140649728854336
	140649728854288 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :         (1, 2)
mat1_sym_strides:         (2, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (2, 32)
mat2_sym_strides:         (1, 2)"]
	140649728854672 -> 140649728854288
	140649728855152 -> 140649728854288
	140649728855152 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649728855104 -> 140649728855152
	140649728855104 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :        (1, 32)
mat1_sym_strides:        (32, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (32, 2)
mat2_sym_strides:        (1, 32)"]
	140649728854960 -> 140649728855104
	140649728855392 -> 140649728855104
	140649728855392 [label="ReshapeAliasBackward0
-----------------------------
self_sym_sizes: (1, 32, 1, 1)"]
	140649728855536 -> 140649728855392
	140649728855536 [label="MaxPool2DWithIndicesBackward0
-----------------------------
ceil_mode  :          False
dilation   :         (1, 1)
kernel_size:       (64, 64)
padding    :         (0, 0)
result1    : [saved tensor]
self       : [saved tensor]
stride     :       (64, 64)"]
	140649815443008 -> 140649728855536
	140649728855344 -> 140649728855104
	140649728855344 [label=TBackward0]
	140649728855200 -> 140649728855344
	140649728855008 -> 140649728854288
	140649728855008 [label=TBackward0]
	140649728855248 -> 140649728855008
	140649815442816 -> 140649815442720
	140649815442816 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649815443344 -> 140649815442816
	140649815443344 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815442912 -> 140649815443344
	140649815442912 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (3, 3)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649728855584 -> 140649815442912
	140649728855584 [label="CatBackward0
------------
dim: 1"]
	140649728855632 -> 140649728855584
	140649728855632 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649728855680 -> 140649728855632
	140649728855680 [label="MaxBackward0
---------------------------
dim       :               1
indices   :  [saved tensor]
keepdim   :           False
self_sizes: (1, 32, 64, 64)"]
	140649815442864 -> 140649728855680
	140649728854816 -> 140649728855584
	140649728854816 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649728855728 -> 140649728854816
	140649728855728 [label="MeanBackward1
-------------------------------
dim           :            (1,)
keepdim       :           False
self          :  [saved tensor]
self_sym_sizes: (1, 32, 64, 64)"]
	140649815442864 -> 140649728855728
	140649728854720 -> 140649815442912
	140649815166144 [label="block_0_2.cbam.spatial_attention.spatial_attention.0.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	140649815166144 -> 140649728854720
	140649728854720 [label=AccumulateGrad]
	140649728853952 -> 140649815443344
	140649815166064 [label="block_0_2.cbam.spatial_attention.spatial_attention.1.weight
 (1)" fillcolor=lightblue]
	140649815166064 -> 140649728853952
	140649728853952 [label=AccumulateGrad]
	140649728854096 -> 140649815443344
	140649815166224 [label="block_0_2.cbam.spatial_attention.spatial_attention.1.bias
 (1)" fillcolor=lightblue]
	140649815166224 -> 140649728854096
	140649728854096 [label=AccumulateGrad]
	140649815442672 -> 140649816735504
	140649815442624 -> 140649815442480
	140649815442624 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815443680 -> 140649815442624
	140649815443680 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649728855824 -> 140649815443680
	140649728855824 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649728855440 -> 140649728855824
	140649728855440 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             32
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649816735504 -> 140649728855440
	140649728855968 -> 140649728855440
	140649815166704 [label="block_0_3.conv_0.depthwise.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	140649815166704 -> 140649728855968
	140649728855968 [label=AccumulateGrad]
	140649728855488 -> 140649728855824
	140649815166864 [label="block_0_3.conv_0.pointwise.weight
 (16, 32, 1, 1)" fillcolor=lightblue]
	140649815166864 -> 140649728855488
	140649728855488 [label=AccumulateGrad]
	140649728854432 -> 140649815443680
	140649815167904 [label="block_0_3.bn_0.weight
 (16)" fillcolor=lightblue]
	140649815167904 -> 140649728854432
	140649728854432 [label=AccumulateGrad]
	140649728854480 -> 140649815443680
	140649815167984 [label="block_0_3.bn_0.bias
 (16)" fillcolor=lightblue]
	140649815167984 -> 140649728854480
	140649728854480 [label=AccumulateGrad]
	140649815442768 -> 140649815442624
	140649815169824 [label="block_0_3.actv_0.weight
 (16)" fillcolor=lightblue]
	140649815169824 -> 140649815442768
	140649815442768 [label=AccumulateGrad]
	140649815442432 -> 140649815442336
	140649815442432 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815442528 -> 140649815442432
	140649815442528 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649728856064 -> 140649815442528
	140649728856064 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649728856160 -> 140649728856064
	140649728856160 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             48
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815442480 -> 140649728856160
	140649728856304 -> 140649728856160
	140649815167024 [label="block_0_3.conv_1.depthwise.weight
 (48, 1, 3, 3)" fillcolor=lightblue]
	140649815167024 -> 140649728856304
	140649728856304 [label=AccumulateGrad]
	140649728856112 -> 140649728856064
	140649815167184 [label="block_0_3.conv_1.pointwise.weight
 (16, 48, 1, 1)" fillcolor=lightblue]
	140649815167184 -> 140649728856112
	140649728856112 [label=AccumulateGrad]
	140649728855776 -> 140649815442528
	140649815168384 [label="block_0_3.bn_1.weight
 (16)" fillcolor=lightblue]
	140649815168384 -> 140649728855776
	140649728855776 [label=AccumulateGrad]
	140649728855296 -> 140649815442528
	140649815168464 [label="block_0_3.bn_1.bias
 (16)" fillcolor=lightblue]
	140649815168464 -> 140649728855296
	140649728855296 [label=AccumulateGrad]
	140649815442576 -> 140649815442432
	140649815169904 [label="block_0_3.actv_1.weight
 (16)" fillcolor=lightblue]
	140649815169904 -> 140649815442576
	140649815442576 [label=AccumulateGrad]
	140649815442288 -> 140649815442192
	140649815442288 [label="PreluBackward0
----------------------
self  : [saved tensor]
weight: [saved tensor]"]
	140649815442384 -> 140649815442288
	140649815442384 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649728856400 -> 140649815442384
	140649728856400 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649728856496 -> 140649728856400
	140649728856496 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :             64
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649815442336 -> 140649728856496
	140649728856640 -> 140649728856496
	140649815167344 [label="block_0_3.conv_2.depthwise.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	140649815167344 -> 140649728856640
	140649728856640 [label=AccumulateGrad]
	140649728856448 -> 140649728856400
	140649815167504 [label="block_0_3.conv_2.pointwise.weight
 (16, 64, 1, 1)" fillcolor=lightblue]
	140649815167504 -> 140649728856448
	140649728856448 [label=AccumulateGrad]
	140649728855920 -> 140649815442384
	140649815168864 [label="block_0_3.bn_2.weight
 (16)" fillcolor=lightblue]
	140649815168864 -> 140649728855920
	140649728855920 [label=AccumulateGrad]
	140649728855872 -> 140649815442384
	140649815168944 [label="block_0_3.bn_2.bias
 (16)" fillcolor=lightblue]
	140649815168944 -> 140649728855872
	140649728855872 [label=AccumulateGrad]
	140649728856352 -> 140649815442288
	140649815169984 [label="block_0_3.actv_2.weight
 (16)" fillcolor=lightblue]
	140649815169984 -> 140649728856352
	140649728856352 [label=AccumulateGrad]
	140649815442144 -> 140649815442048
	140649815167664 [label="block_0_3.conv_3.depthwise.weight
 (80, 1, 3, 3)" fillcolor=lightblue]
	140649815167664 -> 140649815442144
	140649815442144 [label=AccumulateGrad]
	140649815442000 -> 140649815441856
	140649815167824 [label="block_0_3.conv_3.pointwise.weight
 (32, 80, 1, 1)" fillcolor=lightblue]
	140649815167824 -> 140649815442000
	140649815442000 [label=AccumulateGrad]
	140649815441808 -> 140649815441712
	140649815169344 [label="block_0_3.bn_3.weight
 (32)" fillcolor=lightblue]
	140649815169344 -> 140649815441808
	140649815441808 [label=AccumulateGrad]
	140649815441760 -> 140649815441712
	140649815169424 [label="block_0_3.bn_3.bias
 (32)" fillcolor=lightblue]
	140649815169424 -> 140649815441760
	140649815441760 [label=AccumulateGrad]
	140649815441664 -> 140649815441568
	140649815170064 [label="block_0_3.actv_3.weight
 (32)" fillcolor=lightblue]
	140649815170064 -> 140649815441664
	140649815441664 [label=AccumulateGrad]
	140649815441520 -> 140649816735696
	140649815441520 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (1, 32, 1, 1)"]
	140649815441952 -> 140649815441520
	140649815441952 [label="UnsqueezeBackward0
------------------
dim: 3"]
	140649815442096 -> 140649815441952
	140649815442096 [label="UnsqueezeBackward0
------------------
dim: 2"]
	140649815441616 -> 140649815442096
	140649815441616 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649728856256 -> 140649815441616
	140649728856256 [label="SumBackward1
--------------------------
dim           :       (0,)
keepdim       :      False
self_sym_sizes: (2, 1, 32)"]
	140649728856592 -> 140649728856256
	140649728856592 [label="StackBackward0
--------------
dim: 0"]
	140649728856784 -> 140649728856592
	140649728856784 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :         (1, 2)
mat1_sym_strides:         (2, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (2, 32)
mat2_sym_strides:         (1, 2)"]
	140649728856928 -> 140649728856784
	140649815170384 [label="block_0_3.cbam.channel_attention.shared_mlp.3.bias
 (32)" fillcolor=lightblue]
	140649815170384 -> 140649728856928
	140649728856928 [label=AccumulateGrad]
	140649728856880 -> 140649728856784
	140649728856880 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649728857024 -> 140649728856880
	140649728857024 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :        (1, 32)
mat1_sym_strides:        (32, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (32, 2)
mat2_sym_strides:        (1, 32)"]
	140649728857216 -> 140649728857024
	140649815170224 [label="block_0_3.cbam.channel_attention.shared_mlp.1.bias
 (2)" fillcolor=lightblue]
	140649815170224 -> 140649728857216
	140649728857216 [label=AccumulateGrad]
	140649728857168 -> 140649728857024
	140649728857168 [label="ReshapeAliasBackward0
-----------------------------
self_sym_sizes: (1, 32, 1, 1)"]
	140649728857312 -> 140649728857168
	140649728857312 [label="AvgPool2DBackward0
---------------------------------
ceil_mode        :          False
count_include_pad:           True
divisor_override :           None
kernel_size      :       (64, 64)
padding          :         (0, 0)
self             : [saved tensor]
stride           :       (64, 64)"]
	140649815441568 -> 140649728857312
	140649728857120 -> 140649728857024
	140649728857120 [label=TBackward0]
	140649728857456 -> 140649728857120
	140649815170144 [label="block_0_3.cbam.channel_attention.shared_mlp.1.weight
 (2, 32)" fillcolor=lightblue]
	140649815170144 -> 140649728857456
	140649728857456 [label=AccumulateGrad]
	140649728856832 -> 140649728856784
	140649728856832 [label=TBackward0]
	140649728857504 -> 140649728856832
	140649815170304 [label="block_0_3.cbam.channel_attention.shared_mlp.3.weight
 (32, 2)" fillcolor=lightblue]
	140649815170304 -> 140649728857504
	140649728857504 [label=AccumulateGrad]
	140649728856544 -> 140649728856592
	140649728856544 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :         (1, 2)
mat1_sym_strides:         (2, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (2, 32)
mat2_sym_strides:         (1, 2)"]
	140649728856928 -> 140649728856544
	140649728857408 -> 140649728856544
	140649728857408 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140649728857360 -> 140649728857408
	140649728857360 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :        (1, 32)
mat1_sym_strides:        (32, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (32, 2)
mat2_sym_strides:        (1, 32)"]
	140649728857216 -> 140649728857360
	140649728857648 -> 140649728857360
	140649728857648 [label="ReshapeAliasBackward0
-----------------------------
self_sym_sizes: (1, 32, 1, 1)"]
	140649728857792 -> 140649728857648
	140649728857792 [label="MaxPool2DWithIndicesBackward0
-----------------------------
ceil_mode  :          False
dilation   :         (1, 1)
kernel_size:       (64, 64)
padding    :         (0, 0)
result1    : [saved tensor]
self       : [saved tensor]
stride     :       (64, 64)"]
	140649815441568 -> 140649728857792
	140649728857600 -> 140649728857360
	140649728857600 [label=TBackward0]
	140649728857456 -> 140649728857600
	140649728857264 -> 140649728856544
	140649728857264 [label=TBackward0]
	140649728857504 -> 140649728857264
	140649816735648 -> 140649816735552
	140649816735648 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140649815441904 -> 140649816735648
	140649815441904 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140649815441472 -> 140649815441904
	140649815441472 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (3, 3)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140649728857840 -> 140649815441472
	140649728857840 [label="CatBackward0
------------
dim: 1"]
	140649728857888 -> 140649728857840
	140649728857888 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649728857936 -> 140649728857888
	140649728857936 [label="MaxBackward0
---------------------------
dim       :               1
indices   :  [saved tensor]
keepdim   :           False
self_sizes: (1, 32, 64, 64)"]
	140649816735696 -> 140649728857936
	140649728857072 -> 140649728857840
	140649728857072 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140649728857984 -> 140649728857072
	140649728857984 [label="MeanBackward1
-------------------------------
dim           :            (1,)
keepdim       :           False
self          :  [saved tensor]
self_sym_sizes: (1, 32, 64, 64)"]
	140649816735696 -> 140649728857984
	140649728856976 -> 140649815441472
	140649815170624 [label="block_0_3.cbam.spatial_attention.spatial_attention.0.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	140649815170624 -> 140649728856976
	140649728856976 [label=AccumulateGrad]
	140649728856208 -> 140649815441904
	140649815170544 [label="block_0_3.cbam.spatial_attention.spatial_attention.1.weight
 (1)" fillcolor=lightblue]
	140649815170544 -> 140649728856208
	140649728856208 [label=AccumulateGrad]
	140649728856016 -> 140649815441904
	140649815170704 [label="block_0_3.cbam.spatial_attention.spatial_attention.1.bias
 (1)" fillcolor=lightblue]
	140649815170704 -> 140649728856016
	140649728856016 [label=AccumulateGrad]
	140649816735504 -> 140649816735408
	140649816735360 -> 140649816735264
	140649815171104 [label="output_block.conv_1.depthwise.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	140649815171104 -> 140649816735360
	140649816735360 [label=AccumulateGrad]
	140649816735216 -> 140649816735120
	140649815171264 [label="output_block.conv_1.pointwise.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	140649815171264 -> 140649816735216
	140649816735216 [label=AccumulateGrad]
	140649816735072 -> 140649816734976
	140649815171664 [label="output_block.actv_1.weight
 (32)" fillcolor=lightblue]
	140649815171664 -> 140649816735072
	140649816735072 [label=AccumulateGrad]
	140649816734928 -> 140649816734832
	140649815171424 [label="output_block.conv_2.depthwise.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	140649815171424 -> 140649816734928
	140649816734928 [label=AccumulateGrad]
	140649816734544 -> 140649816734592
	140649815171584 [label="output_block.conv_2.pointwise.weight
 (3, 32, 1, 1)" fillcolor=lightblue]
	140649815171584 -> 140649816734544
	140649816734544 [label=AccumulateGrad]
	140649816734784 -> 140649816734736
	140649815171744 [label="output_block.actv_2.weight
 (3)" fillcolor=lightblue]
	140649815171744 -> 140649816734784
	140649816734784 [label=AccumulateGrad]
	140649816734448 -> 140649815371712
}
