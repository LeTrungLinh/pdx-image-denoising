{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, Sampler\n",
    "from skimage.util import view_as_windows\n",
    "from utils import load_image\n",
    "from transforms import ToTensor\n",
    "import cv2\n",
    "import yaml\n",
    "from os.path import join\n",
    "from transforms_test import RandomHorizontalFlip, RandomVerticalFlip, RandomRot90\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ham con "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patches(image, patch_size, step):\n",
    "    image = view_as_windows(image, patch_size, step)\n",
    "    h, w = image.shape[:2]\n",
    "    image = np.reshape(image, (h * w, patch_size[0], patch_size[1], patch_size[2]))\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveWhiteGaussianNoise(object):\n",
    "    \"\"\"Additive white gaussian noise generator.\"\"\"\n",
    "    def __init__(self, noise_level, fix_sigma=False, clip=False):\n",
    "        self.noise_level = noise_level\n",
    "        self.fix_sigma = fix_sigma\n",
    "        self.rand = np.random.RandomState(1)\n",
    "        self.clip = clip\n",
    "        if not fix_sigma:\n",
    "            self.predefined_noise = [i for i in range(5, noise_level + 1, 5)]\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        Generates additive white gaussian noise, and it is applied to the clean image.\n",
    "        :param sample:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        image = sample.get('image')\n",
    "\n",
    "        if image.ndim == 4:                 # if 'image' is a batch of images, we set a different noise level per image\n",
    "            samples = image.shape[0]        # (Samples, Height, Width, Channels) or (Samples, Channels, Height, Width)\n",
    "            if self.fix_sigma:\n",
    "                sigma = self.noise_level * np.ones((samples, 1, 1, 1))\n",
    "            else:\n",
    "                sigma = np.random.choice(self.predefined_noise, size=(samples, 1, 1, 1))\n",
    "            noise = self.rand.normal(0., 1., size=image.shape)\n",
    "            noise = noise * sigma\n",
    "        else:                               # else, 'image' is a simple image\n",
    "            if self.fix_sigma:              # (Height, Width, Channels) or (Channels , Height, Width)\n",
    "                sigma = self.noise_level\n",
    "            else:\n",
    "                sigma = self.rand.randint(5, self.noise_level)\n",
    "            noise = self.rand.normal(0., sigma, size=image.shape)\n",
    "\n",
    "        noisy = image + noise\n",
    "        \n",
    "        if self.clip:\n",
    "            noisy = np.clip(noisy, 0., 255.)\n",
    "        for i in range(len(noisy)):\n",
    "            plt.imshow(noisy[i].astype('float32'))\n",
    "            plt.show()\n",
    "            \n",
    "        return {'image': image, 'noisy': noisy.astype('float32')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyImagesDataset(Dataset):\n",
    "    def __init__(self, files, channels, patch_size, transform=None, noise_transform=None):\n",
    "        self.channels = channels\n",
    "        self.patch_size = patch_size\n",
    "        self.transform = transform\n",
    "        self.noise_transforms = noise_transform\n",
    "        self.to_tensor = ToTensor()\n",
    "        self.dataset = {'image': [], 'noisy': []}\n",
    "        self.load_dataset(files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset['image'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, noisy = self.dataset.get('image')[idx], self.dataset.get('noisy')[idx]\n",
    "        sample = {'image': image, 'noisy': noisy}\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        sample = self.to_tensor(sample)\n",
    "\n",
    "        return sample.get('noisy'), sample.get('image')\n",
    "\n",
    "    def load_dataset(self, files):\n",
    "        patch_size = (self.patch_size, self.patch_size, self.channels)\n",
    "        for file in tqdm(files):\n",
    "            image = load_image(file, self.channels)\n",
    "            if image is None:\n",
    "                continue\n",
    "\n",
    "            image = create_patches(image, patch_size, step=self.patch_size)\n",
    "            sample = {'image': image, 'noisy': None}\n",
    "\n",
    "            for noise_transform in self.noise_transforms:\n",
    "                _sample = noise_transform(sample)\n",
    "                image, noisy = _sample['image'], _sample['noisy']\n",
    "\n",
    "                image, noisy = list(image), list(noisy)\n",
    "\n",
    "\n",
    "                self.dataset['image'].extend(image)\n",
    "                self.dataset['noisy'].extend(noisy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as stream:                # Load YAML configuration file.\n",
    "        config = yaml.safe_load(stream)\n",
    "\n",
    "model_params = config['model']\n",
    "train_params = config['train']\n",
    "val_params = config['val']\n",
    "with open('train_test.txt', 'r') as f_train, open('val_files.txt', 'r') as f_val:\n",
    "    raw_train_files = f_train.read().splitlines()\n",
    "    raw_val_files = f_val.read().splitlines()\n",
    "    train_files = list(map(lambda file: join(train_params['dataset path'], file), raw_train_files))\n",
    "    val_files = list(map(lambda file: join(val_params['dataset path'], file), raw_val_files))\n",
    "training_transforms = transforms.Compose([\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomVerticalFlip(),\n",
    "        RandomRot90()\n",
    "    ])\n",
    "train_noise_transform = [AdditiveWhiteGaussianNoise(train_params['noise level'], clip=True)]\n",
    "training_dataset = NoisyImagesDataset(train_files,\n",
    "                                          model_params['channels'],\n",
    "                                          train_params['patch size'],\n",
    "                                          training_transforms,\n",
    "                                          train_noise_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.transforms import ToPILImage\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt \n",
    "# # Assuming you have the tensor as `image_tensor`\n",
    "# for i in range(117):\n",
    "#     img_test = training_dataset[i][1]*255\n",
    "#     image_np = img_test.detach().numpy()\n",
    "#     image_np = np.transpose(image_np, (1, 2, 0)) # to convert from (3, 64, 64) to (64, 64, 3)\n",
    "#     image_np_uint8 = (255*image_np).astype(np.uint8)\n",
    "\n",
    "#     # image_pil = ToPILImage()(image_np_uint8)\n",
    "#     # image_pil.show()\n",
    "#     print(type(image_np_uint8))\n",
    "#     plt.imshow(image_np_uint8)\n",
    "#     plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test lightweight model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, chs=(3,64,128,256)):\n",
    "        super().__init__()\n",
    "        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
    "        self.pool       = nn.MaxPool2d(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ftrs = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            ftrs.append(x)\n",
    "            x = self.pool(x)\n",
    "        return ftrs\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, chs=(256, 128, 64)):\n",
    "        super().__init__()\n",
    "        self.chs        = chs\n",
    "        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])\n",
    "        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)]) \n",
    "        \n",
    "    def forward(self, x, encoder_features):\n",
    "        for i in range(len(self.chs)-1):\n",
    "            x        = self.upconvs[i](x)\n",
    "            enc_ftrs = encoder_features[i]\n",
    "            x        = torch.cat([x, enc_ftrs], dim=1)\n",
    "            x        = self.dec_blocks[i](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    '''\n",
    "    Simple UNet-like model \n",
    "    Input: RGB image. For S7 (504, 504, 3), and for P20 (496, 496, 3) \n",
    "    \n",
    "    Outputs: RAW image as 4-channels (H // 2, W // 2, 4) following RGGB pattern.\n",
    "             For the S7 output should be (252, 252, 4), and for HP20  (248, 248, 4)\n",
    "    '''\n",
    "    def __init__(self, enc_chs=(3,64,128,256), dec_chs=(256, 128, 64), out_ch=3, out_sz=(504, 504)):\n",
    "        super().__init__()\n",
    "        self.encoder     = Encoder(enc_chs)\n",
    "        self.decoder     = Decoder(dec_chs)\n",
    "        self.head        = nn.Conv2d(dec_chs[-1], out_ch, 1)\n",
    "        self.out_sz      = out_sz\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_ftrs = self.encoder(x)\n",
    "        out = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
    "        out = self.head(out)\n",
    "        out = F.interpolate(out, self.out_sz)\n",
    "        out = torch.clamp(out, min=0., max=1.)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Encoder: 1-1                           --\n",
      "|    └─ModuleList: 2-1                   --\n",
      "|    |    └─Block: 3-1                   38,592\n",
      "|    |    └─Block: 3-2                   221,184\n",
      "|    |    └─Block: 3-3                   884,736\n",
      "|    └─MaxPool2d: 2-2                    --\n",
      "├─Decoder: 1-2                           --\n",
      "|    └─ModuleList: 2-3                   --\n",
      "|    |    └─ConvTranspose2d: 3-4         131,200\n",
      "|    |    └─ConvTranspose2d: 3-5         32,832\n",
      "|    └─ModuleList: 2-4                   --\n",
      "|    |    └─Block: 3-6                   442,368\n",
      "|    |    └─Block: 3-7                   110,592\n",
      "├─Conv2d: 1-3                            195\n",
      "=================================================================\n",
      "Total params: 1,861,699\n",
      "Trainable params: 1,861,699\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Encoder: 1-1                           --\n",
       "|    └─ModuleList: 2-1                   --\n",
       "|    |    └─Block: 3-1                   38,592\n",
       "|    |    └─Block: 3-2                   221,184\n",
       "|    |    └─Block: 3-3                   884,736\n",
       "|    └─MaxPool2d: 2-2                    --\n",
       "├─Decoder: 1-2                           --\n",
       "|    └─ModuleList: 2-3                   --\n",
       "|    |    └─ConvTranspose2d: 3-4         131,200\n",
       "|    |    └─ConvTranspose2d: 3-5         32,832\n",
       "|    └─ModuleList: 2-4                   --\n",
       "|    |    └─Block: 3-6                   442,368\n",
       "|    |    └─Block: 3-7                   110,592\n",
       "├─Conv2d: 1-3                            195\n",
       "=================================================================\n",
       "Total params: 1,861,699\n",
       "Trainable params: 1,861,699\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = torch.zeros((1, 3, 504, 504))\n",
    "model = UNet()\n",
    "y = model(xx)\n",
    "# print(y[0].detach().numpy()[0])\n",
    "summary(model,input_size=(3, 504, 504))\n",
    "# print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr_env",
   "language": "python",
   "name": "ocr_env"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
